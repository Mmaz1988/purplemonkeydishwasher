\documentclass[color]{pbml}
%\documentclass[nofonts]{pbml} % for XeLaTeX without Pagella and DejaVu fonts
%\documentclass[color]{pbml} % for color images and hypertext links

\usepackage{expex}

\begin{document}

\title{Morphological Segments in SMT and RBMT\titlelinebreak{}
Morphological Joining and Splitting with English to Finnish MT systems}


\institute{}{CNGL---School of Computing, Dublin City University}

\author{
  firstname=Tommi,
  initials=A,
  surname=Pirinen,
  corresponding=yes,
  email={tommi.pirinen@computing.dcu.ie},
  address={CNGL---School of Computing\\
  Dublin City University\\
  Glasnevin, Dublin 9, Dublin, Ireland}
}

\shorttitle{English to Finnish MT Experiments}
\shortauthor{T. A Pirinen}

\PBMLmaketitle


\begin{abstract} 
    In this paper we performed an experiment on standard moses
    style statistical phrase-based machine translation from English to Finnish
    using morphs and compound word segments. The target language model and the
    translation model is trained based on pre-processed segmented corpus and
    the resulting translations are joined. For segmentation we use
    state-of-the-art weighted finite-state based morphological analyser trained
    with statistical morphs and word-form data from corpus. The joining was
    performed using simple baseline joining method.  We compared results to
    baseline europarl-trained baseline SMT and the state-of-the-art rule-based
    machine-translation and notice that our method gives slight improvements.
    The result is presented as a ready-to-use end-to-end moses-based translation
    system wrapped in easy-to-use free and open source autotools-based package.
\end{abstract}

\section{Introduction}

Morphological splitting and joining is an important part of the machine
translation with morphologically complex languages. Morphologically complex
languages cause problems for statistical natural language processing:
the potential amount of different tokens is infinite and the practical amount
of unique tokens is much larger, requiring more training data to have statistics
with discriminative power. Another issue is that more of the alignments between
morphologically different languages are one to many, and may be split far apart
in the translation versions. The way we propose to work on it is to base the
translation on words or morphs of the morphologically rich language, which are
(mostly) finite set of tokens and typically align better.

When working in the
direction of morphologically poor language to morphologically complex language
where productive morphology is rich and common, there are two processes that
need to be done: firstly the segmentation of the training data prior to
training target language model and translation model (pre-processing step),
secondly the joining of the translated segments back to legal morphological
combinations (post-processing step). This is the direction we are exploring in
this paper, for the other direction, the joining step is not necessary, but the
selection and formulating the segmentation may be more important, as it may be
required as the pre-processing step of the input of the final system.  The
segmentation of training data to compound parts or morphs can be done as a
preprocessing step.

\citet{cap2014produce} has described a method for combining
CRF-based (conditional random field) statistical compound joiner to
German-Russian statistical machine translation.
\citet{fishel2010linguistically} has shown that for Estonian to English the
morphological segmentation shows promising increase in scores.

In this paper we create a readily usable system from the state-of-the-art
methods of morphological splitting, combined with baseline joining and
statistical phrase-based and rule-based machine translation systems using
Finnish and English as the experiment pair. For the baseline in SMT we have
selected moses~\citep{}, and we use the same basic system for all our SMT
experiments.  For RBMT reference we use apertium~\citep{forcada2011apertium}
and the Finnish---English translation pair̃\footnote{\url{SVN URL}}.  The
experiments we make and data needed for reproducing the results are available
freely in our github repository\footnote{\url{}}.

The article is structured as follows: first we show the methods used and 
rationale for them in section~\ref{sec:methods}, then we describe the data used
for training and testing briefly in section~\ref{sec:data}, then we show
standard evaluations in section~\ref{sec:evaluation}, then we show error
analysis and compare our methods to other research in section~\ref{sec:discussion} and finally we conclude how the experiment matches our initial hypothesis in
section~\ref{sec:conclusion}.

\section{Methods}
\label{sec:methods}

The basis of the translation system built here is standard moses~\citep{koehn}.
To create a baseline we have used the instructions on the Moses baseline
Wikipage,\footnote{\url{http://www.statmt.org/moses/?n=moses.baseline}} except
for the tuning part or minimum error rate training (MERT), which we left out
due to lack of time. The segmented models are built in exactly the same way
with the exception that the corpus is pre-processed with the 1-best
segmentation by the rule-based language model.

For the state-of-the-art of morphological segmentation we have selected a
combination of rule-based methods using finite-state morphological
descriptions~\citep{beesley2003finite,pirinen2010}. The systems are able to
turn input word-forms into list of possible segmentations.  Because
segmentations can some times be ambiguous, we used the state-of-the-art
weighted finite-state automata (WFSA) methods as described
by~\citet{pirinen2010weighting}. They used words of corpus as is to weight
segments of an analysing automaton, we have made only very minor modifications
to the algorithm to make it work with a segmentation automaton of compounds,
such as changing segment markers to include spaces so moses can pick it up. For
morph segments, we did the same, but instead of words of the corpus we used
automatically segmented character sequences made by unsupervised morphological
segmentation system Morfessor~\cite{virpioja2014morfessor}. The basic logic of
the training here is to use the frequencies of the tokens in the corpus as
usual transforming it to frequency weighted automaton. The frequency weighted
automaton is composed over the rule-abased segmentation automaton to add
weights to the segments that are morphologically plausible. The rest of the
automaton is weighted by giving maximum weight to each segment so unseen tokens
have greater weight than anything found in corpus. The weights and weight
distribution follows standard WFSA tropical semiring with add-one smoothing.
More formally:

\begin{equation}
    w(\hat x) = -\log \frac{x + \alpha}{\alpha \times \mathrm{VS} + \mathrm{CS}},
\end{equation}

\noindent where $x$ is a word-form, $w(\hat x)$ is the end weight of the word-form in the
automaton, $\alpha$ the constant for additive smoothing, i.e., $1$ in our
experiment, $\mathrm{VS}$ the vocabulary size, or the count of different unique
tokens, and $\mathrm{CS}$ the corpus size, or the count of the tokens. This
leaves us with the estimated weight of $-\log\frac{\alpha}{\alpha \times \mathrm{VS} + \mathrm{CS}}$ for the unseen tokens. 

The basic logic behind the morphological segmentation for machine translation
is presented in figure~\ref{fig:alignments}, which corresponds to the
linguistic examples in~(\nextx). This shows that while prototypical case
\emph{sta} (elative) is aligned with the English preposition from, it can act
with some verbs as a marker that corresponds e.g., to English subject or
object. So, in the prototypical cases of morphs there's a neat 1-to-1 alignment
between Finnish morphs and English words. There are still cases of 1-to-many or
1-to-0 alignments but we expect that since these are dependent on local context
such as verb requiring specific case for its object, it will still work.

\pex
\a
    \begingl
    \gla koira-sta //
    \glb dog-{\sc Ela} //
    \glft `from the dog' //
    \endgl
\a
    \begingl
    \gla pidä-n koira-sta //
    \glb like-{\sc Sg1} dog-{\sc Ela} //
    \glft `I like the dog (lit. I like from dog) //
    \endgl
\a
    \begingl
    \gla koira-sta on hauska leikki-ä //
    \glb dog-{\sc Ela} be fun play-{\sc InfA} //
    \glft `dog likes to play (lit. from dog is fun to play)' //
    \endgl
\xe

\begin{figure}
    \begin{center}
        \includegraphics[trim=0 550 0 0,clip,width=\textwidth]{alignments}
    \end{center}
    \caption{Alignments of morph \emph{sta} in Finnish
    \label{fig:alignments}}
\end{figure}

The training of translation models for segmented data is the same as with 
space-separated word-forms, however, the result of translation needs to joined
back from the segments. For this baseline test we have merely joined adjacent
segments when they've had matching special symbols for left and right
segmentation boundaries; all stray segments were removed from the stream. That
is, e.g., segmentation of \emph{abc} to \emph{a b c} will join \emph{a b}, or
\emph{b c} but also \emph{a c}, however, \emph{cba} would be fully deleted
since c must join on left and not right, b on both sides but neither c or a
allows it here, and a on right and not left.

\section{Data}
\label{sec:data}

For the training of all of the components in this experiment we use 
europarl~\citep{koehn2005europarl}, partly because that is the corpus mentioned
in the baseline moses documentation, partly because it is the only free and open
source corpus for Finnish---English language pair. The data was split into
training part, development part and test part in ratio of 8/1/1, and while the
development part was left unused for this experiment, we will revise the system
for future projects so we've used to the standard split of the data.

\section{Evaluation}
\label{sec:evaluation}

For evaluation we use two texts, the held out part of the europarl corpus and
the united nations declaration of human rights. The tests we carry out are the
standard BLEU~\citep{} using the NIST 13b testing script~\footnote{} and the
TER~\citep{} using the TERcom java program.\footnote{} The results are given in
the table~\ref{table:results}.

\begin{table}
    \begin{center}
        \begin{tabular}{|l|r|r||r|r|}
            \hline
            Score: & \bf BLEU & \bf TER  & \bf BLEU  & \bf TER   \\
            Model  & (in dom) & (in dom) & (out dom) & (out dom) \\ 
            \hline
            \bf Apertium Baseline      &  & & 0.01 & 0.96 \\
            \hline
            \bf Moses Baseline         &  & & 0.16 & 0.71 \\
            \hline
            \bf Moses 1-Best Compounds &  & & 0.18 & 0.70 \\
            \hline
            \bf Moses 1-Best Morphs    &  & & 0.12 & 0.74 \\
            \hline
        \end{tabular}
        \caption{Test results for BLEU and TER
        \label{table:results}}
    \end{center}
\end{table}

\section{Discussion}
\label{sec:discussion}

We have presented a system for using rule-based morphology with some sort of
segmentations as a basis for statistical phrase-based machine translation from
morphologically poor language to morphologically rich one. Using previous work
on rule-based morphological segmentation we have managed to get slight
improvement to the translation quality in some parts, which is in line with
results of̃~\citet{fishel} but this is not well reflected in BLEU, as is also
mentioned by~\citet{virpioja}.

When we take a look at improvements and errors, we notice, that \ldots

In this experiment we used relatively crude system for joining the separated
morphs. For future research it would be interesting to employ more
sophisticated methods for the task. It would be interesting to see the results
with linguistically guided one, e.g., one used by~\citet{cap2014unseen}. This
might get rid of such combinations as wrongly combined noun phrases, or wrong
object and subject forms.

\section{Conclusions}
\label{sec:conclusion}

In this experiment we set out to implement an end-to-end English-to-Finnish
machine translation system based on morphological segmentations and estimated
it will improve the baseline machine translation results for this language pair
which is known to be difficult. We notice that the segmentation gives some 
improvements

\section*{Acknowledgements}
This research is supported by Marie Curie Actions and \ldots

\bibliography{mtm2014}

\correspondingaddress
\end{document}


% vim: set spell:
