\documentclass[a4paper,12pt]{article}

\newif\iffinal
\finalfalse

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{expex}

\usepackage{url}
\usepackage{hyperref}

\usepackage[margin=3cm]{geometry}

\usepackage{natbib}
\setlength{\bibsep}{0pt}
\linespread{1.3}

\defaultfontfeatures{Mapping=tex-text}
\setmainfont{Times New Roman}

\usepackage{titlesec}

\titleformat*{\section}{\bfseries}


\title{Omorfi---Open-Source Finite-State Morphological Lexicon of Finnish}
\iffinal
\author{Tommi A Pirinen \\
    Dublin City University\\
    ADAPT---School of Computing}
\fi
\date{}
\begin{document}

\maketitle
\begin{abstract}

    This squib describes a contemporary system for the computational modeling of
    the morphology of Finnish word-forms. The system we present is at its core a
    finite-state morphology, just like other popular systems for computational
    Finnish morphology have been since 1983.  The purpose of this squib is to
    describe the key differences against the earlier analysers, and present
    a new baseline system for researchers of Finnish. We have experimented with
    some of the recent ideas from the field of statistical computational
    linguistics to pave a road for a full-fledged hybrid rule-based and
    statistical finite-state system for morphological modeling. We also present
    a selection of real-world use cases for the morphological analysers that
    have affected the development of omorfi.

    Our system is based fully on free and open source
    data and methods. This allows for some new
    approaches to the lexicographical data collection
    using crowd\-sour\-ced dictionary Wiktionary, as
    well as statistical modelling with e.g., the
    encyclopedia Wikipedia data.

    To evaluate our analyser, we have processed large text corpora with it
    and verified that only 1~\% of the word-forms remains unknown to the
    system. Omorfi is also verified to match the FinnTreeBank 3.1 standard
    analyses at 90~\% faithfulness per token count for interoperability.
\end{abstract}

\section{Introduction}

Computational morphological models are a central component for most of the
computational applications of linguistic analysis. Computational morphology
of the Finnish language was first described some 30 years
ago~\citep{koskenniemi1983twolevel}. The aim of this paper is to present the
newest approach to the task and examine the differences of our current approach
to traditional systems, and to highlight some of the new development. The
new development that we discuss is mainly in line with the recent trends in
the field of computational linguistics. We also present some of the current
use-cases of the system.

One of the main goals of this paper is to act as the
central scientific documentation of our current
morphological analyser, and we have made an attempt
to highlight the long-term design goals of the system
instead of transitional and volatile features of a
fast-moving computer software that is developed by a
base of open-source and language enthusiasts. On the
other hand, this is also a scientific squib, so we
describe shortly the most recent modifications made
to the rule-based component, and the additions in the
dictionary data model we made to accommodate the
semi-automatic crowd-sourced lexicography using
Wiktionary as a lexical data source.  We also refer
to further use of omorfi by numerous researchers for
various tasks, such as
spell-checking~\citep{pirinen2014weighted}, language
generation~\citep{toivanen2012corpus}, machine
translation~\cite{clifton2011combining}, and
statistical
models~\citep{haverinen2013building,bohnet2013joint}.
We consider this as additional evidence of the
usability of our approach to the weighted finite-state
morphological language modeling.

The statistical approach to language modeling has
been popular throughout 2000's, however, with
morphologically complex languages it brings
additional challenges since data is scarce and
requirement for word-forms to get discriminative
probabilities is higher.\footnote{We assume here that
    morphological complexity necessarily means more
    word-forms per word, while this may not always be
the case, it is true for Finnish} In omorfi we have
started to adapt the traditional rule-based approach
towards a hybrid approach where the morphological
analyser is defined in the traditional manner using
inflection paradigm classified dictionary and rules
in style of~\citet{koskenniemi1983twolevel}, and the
result is trained using frequencies of word-forms
found in the corpus. We show some combinations of
contemporary methods of combining the rule-based
analysers with statistical data, such as ones
described by~\citet{pirinen2009weighted}
and~\citet{pirinen2012improving}.  While the basic
unigram models shown in the articles and used by the
system are merely a modest baseline built for
specific applications, they show a proof of concept
for developers of other systems to build more modern
statistical structures upon.


Another recent development in the computational language models is the concept
of \emph{maintainability} of these computational systems, e.g.
by~\cite{maxwell2008joint,pirinen2011modularisation}. Specifically we will show
how we use the power of \emph{crowd-sourcing} to keep up with the new words,
neologisms and other rare words missing from dictionaries. In particular we
study the use of the popular online dictionary Wiktionary as a source of
additional lexical data. To support semi-automatic use of Wiktionary data we
have devised a morph-based formulation of the finite-state morphology of
Finnish which we test in conjunction with basic classification schemes to help
users to classify words.

One notable practical distinction in our system is its licensing policy. Omorfi
analyser is a free and open source product.\footnote{
\url{http://code.google.com/p/omorfi/}}  In contemporary computational
linguistics, freeness of systems and data is rightly seen as a cornerstone of
properly conducted science, as it fulfills the requirement of repeatability by
not setting unnecessary fences for the repetition of the scientific results.
There is a large base of recent research supporting this, specifically for
Finnish the latest is by~\citet{koskenniemi2008build}. For computer-savvy end
users, such as language technologists, this means that the tools necessary to
perform linguistic analysis with omorfi can be downloaded to any average PC,
and for the rest of the scientists we co-maintain a usable installation at
CSC.\footnote{\url{http://www.csc.fi/english/research/sciences/linguistics}, 
for universities outside Finland you may be able to convince your local data
center to install accessible version.}

To summarise, this article is an overview of a new Finnish morphological
lexicon.  It consists of the methodological advances found from past years
research of finite-state morphologies and computational linguistics, and
contrasts the resulting system to the previous state-of-the-art in
morphological lexicon of Finnish. It also shows some recent uses of a
morphological models in real world applications. We also show how
crowd-sourced lexicography can be used as a part of the system.

\section{Methods}
\label{sec:methods}

There have been numerous implementations of
computational morphological analysis of Finnish along
the years. These  can be dated back to at least
1980's, including~\citet{koskenniemi1983twolevel},
which is among the more influential works in the
field of computational morphology.  Otherwise, the
implementation of our analyser follows the
traditional works on finite-state morphology
by~\citet{beesley2003finite}. On top of that we have
applied recent extensions from the research of
finite-state morphology, such as weighted
finite-state methods~\citep{openfst,hfst2012}. What
this means in practice is basic unigram probabilities
of word-forms composed\footnote{using the
finite-state algebra operation composition that is
well-defined in terms of likelihood-based automata}
over the analyser from a corpus.  This brings the
traditional rule-based language analyser towards the
statistical language analysers that are widely
popular in the handling of morphologically less
complex languages.

A rule-based analyser is implemented based on the classification of the words
by their inflectional patterns. The patterns in Finnish are based on two parts:
stem variations and suffix allomorphs. The classified dictionary words are
stripped of their varying stem parts, and then concatenated with the variations
and then stems by means of standard finite-state morphology. E.g. dictionary
word \emph{vesi} `water' has stem variation in \emph{-si} $\sim$ \emph{-te-}
$\sim$ \emph{-de-} $\sim$ \emph{-t-}, and respectively suffixes $\emptyset$
(nominative) $\sim$ \emph{-nä} (essive, `as water') $\sim$ \emph{-ssä}
(inessive `in water')  $\sim$ \emph{-tä} (partitive, roughly `of water')
arranged in morph sublexicons\footnote{the word morph is used here on purpose
    as opposed to morpheme because the default implementation has no
morphophonemic level whatsoever} as in \emph{Finite State
Morphology}~\citep{beesley2003finite}. Shortly said, such finite-state
formulation of morphotactics is a set of disjunctions of paths in the automaton
representing the morphs. The morphs are extended by their classifications, and
the morphotactic rules over classifications are enforced using composition
operation, such as: noun root morphs are followed by case morphs and then
optionally possessives and clitics~\citep{hfst2012}. The practical
implementation is thus based on a database of morphs (including root morphs)
and their combinatorics, converted into lexc~\citep{beesley2003finite}
structure, and combiled with \texttt{hfst-lexc} into a finite-state automaton.
No morphophonology or variation rules are applied in the process, while
database has some morphophonological representations available, there has not
been seen a need to use them with current set of applications as the simple
morph-concatenations seem to describe the system accurately.

The baseline statistical methods for morphological models are applied here in a
straight-forward manner; get the likelihood $P(w)$ for the surface form $w$, by
counting the amount of word-forms $f(w)$ in a corpus and divide it by the
number of word-forms in the whole corpus $CS$: $P(w) = \frac{f(w)}{CS}$.  To
get around the problems with the probability of $0$ for unseen word-forms, we
use additive smoothing~\citep{chen1999empirical}, which estimates likelihood of
each type as $1$ larger than it is and size of corpus as number of types
larger. The acquired likelihoods are combined to the rule-based morphological
analyser using basic finite-state composition of the weighted disjuncted
word-form paths over the surface side of the analyser. Possible extension to
that comes from the recent research on training of the compounding language
models~\citep{pirinen2009weighting} to use the estimation of the compound words
by their parts. This means that given unseen compound \emph{banaaniovi} `banana door', the
estimated likelihood is $P(\mathrm{banaani}) \times P(\mathrm{ovi})$, rather
than $P(\mathrm{banaaniovi})$. One of the non-apparent results of such training
is that in case of ambiguous compounds, the analyser will systematically prefer
the ones with least words. The current version provides an application of
surface weights like these as an option, given a suitable corpus is provided
during compilation. Similarly given a corpus of disambiguated analyses, the
same form of probabilities could be applied on the analyser for the full
analysis strings, or part of them in similar manner to create a baseline
disambiguating analyser. It is also entirely possible to build models with more
advanced statistical methods, such
as~\citet{opengrm}.\footnote{\url{http://opengrm.org}} The version being
documented here that is released and available~\footnote{\url{https://code.google.com/p/omorfi/source/list?name=omorfi-20141014}---it may not be the newest by the time of publication.},
includes only surface form unigram model with additive smoothing and simple
rule-based disambiguation logic, as that was sufficient to implement current
end-applications, such as spell-checking and non-disambiguating analyser. It is
likely that other combinations will be available in due course as applications
and corpora for those become more available.


To help the collection of new lexical data, we have used some automatic
classification methods, such as longest common suffix search from the
morphological dictionary.  The way it works is that given a new word-form, it
can go through the dictionary to find a longest suffix it has in common with
some existing word-form. For example  given a new word-form \emph{googlella}
`with google', the system can guess that it might inflect like \emph{nallella} `with
a teddy bear', or \emph{puolella} `with a half' (common suffix of 5
characters), and following our classification then suggest the end user or corpus
harvester to study forms like: \emph{googlen, googlea, googleen, googlet,
googlejen, \ldots} versus \emph{googlen, ${}^\star$googlta, googleen, googlet,
${}^\star$googlten, \ldots} to decide on the classification of the new word. To
help this semi-automatic classification we have reformulated morpheme-based and
phonologically expanded finite-state morphologies as plain morph-based
morphology.

\section{Data}
\label{sec:data}

There are a few freely available open resources for lexicographical data of
Finnish. The first one  we used is based on a dictionary maintained by (Research)
institute of languages in Finland (RILF). The lexicographical data of the
dictionary has been available under free software licence LGPL since 2007 as
\emph{Nykysuomen
sanalista.}\footnote{\url{http://kaino.kotus.fi/sanat/nykysuomi}}.  The second
source of lexical data we acquired from the internet is a free, open source
database named
\emph{Joukahainen.}\footnote{\url{http://joukahainen.puimula.org}}  For another
source of lexical data we use the popular crowd-sourced \emph{Wiktionary}
project.  Finally, we have used data that has been produced for various
projects at university of Helsinki, including other lexical projects, such as
FinnWordNet~\citep{linden2010finnwordnet}.\footnote{The primary sources for
lexical data used in these projects include europarl corpus and English
wordnet, but not any of the corpora we have used for evaluations: Wikipedia,
JRC acquis or Gutenberg.} The classification scheme and precision between
these sources vary, so the semi-automatic classification method specified in
section~\ref{sec:methods} has been useful, as evidenced first
by~\citet{listenmaa2009combining} and numerous contributors thereafter.

The full lexical data used in the current version of our analyser consists of
396,673 classified lexemes, the classification is roughly summarised by word's
classes and their origin in table~\ref{table:lexical}. 

\begin{table}
    \begin{scriptsize}
  \centering
    \begin{tabular}{|l|r|r|r|r||r|}
        \hline
        \bf Database: & Kotus & Joukahainen & Wiktionary & UH & \bf Total \\
        \bf Class   & & & & & \\
        \hline
        Adjectives     & 10,537 & 652   & 59    & 6,368   & 17,616  \\
        Nouns          & 67,608 & 2,945 & 1,133 & 1,589   & 73,275  \\
        (Proper nouns) & 20     & 5,394 & 70    & 262,330 & 267,814 \\
        Verbs          & 9,685  & 476   & 12    & 499     & 10,672  \\
        Others         & 6620   & 5     & 12    & 25,536  & 32,173  \\
        \hline
        \bf Total      & 94,290 & 9,472 & 1,286 & 296,322 & 396,637 \\
        \hline
    \end{tabular}
  \caption{Lexical data used in the analyser.  The columns are the lexical
      sources used: Nykysuomen sanalista in column titled \emph{Kotus} (RILF),
      \emph{Joukahainen}, \emph{Wiktionary}, terms harvested in various
      research project in University of Helsinki in column titled \emph{UH},
      and a total sum in the last column. The rows represent coarse
      morphological classification, with proper nouns separated into their own
      row. On the \emph{others} row are the adverbs, adpositions and such words
      with defective paradigms. These other words in current dictionaries have one
      entry per inflectional ending.
  \label{table:lexical}}
  \end{scriptsize}
\end{table}


For evaluation we use only freely available corpora. The size of these is
detailed in table~\ref{table:corpora}. The corpora are following: ebooks of project
Gutenberg\footnote{\url{http://gutenberg.org}}, the data of Finnish
Wikipedia\footnote{\url{http://fi.wikipedia.org}}, and the JRC acquis
corpus\footnote{\url{http://ipsc.jrc.ec.europa.eu/index.php?id=198}}. For
downloading and preprocessing these corpora we use freely available
scripts~\footnote{\url{https://github.com/flammie/bash-corpora}}. The scripts
retain most of the punctuation and white-space as-is. The resulting
token counts are given in table~\ref{table:corpora}. Some further tests were made
with fully tokenised and analysed
FinnTreeBank~\citep{voutilainen2012specifying} version 3.1.  The scripts used
for evaluation are part of omorfi source code.

\begin{table}
    \begin{scriptsize}
  \centering
    \begin{tabular}{|l|r|r|}
        \hline
        \bf Feature: & Tokens     & Types     \\
        \bf Corpus   &            &           \\
        \hline
        Gutenberg    & 36,743,872 & 1,590,642 \\
        Wikipedia    & 55,435,341 & 3,223,985 \\
        JRC Acquis   & 42,265,615 & 1,425,532 \\
        FTB          & 76,369,439 & 1,648,420 \\
        \hline
    \end{tabular}
  \caption{Corpora used for evaluations. Tokens are all strings extracted from
      corpus and types are unique strings, both include punctuation and some
      codified expressions like URLs, addresses etc.
  \label{table:corpora}}
  \end{scriptsize}
\end{table}

\section{Evaluation}
\label{sec:evaluation}

In this section we evaluate our analyser using very basic methods of evaluation
and free open source data. Firstly, we measure a naive coverage by trying to
analyse large corpora and see the proportion of word-forms that can be analysed
at all. Then we measure the reproduction capability for a reference corpus of
morphologically analysed and disambiguated texts. To match omorfi analyses with
other analyser's output we used a python script that basically does naive
string matching. The version of omorfi we used was tagged version
20141014\footnote{\url{https://code.google.com/p/omorfi/source/list?name=omorfi-20141014}}
configured with default settings.


First we measure how big proportion of data in the material are
out-of-vocabulary items. This gives us naive coverage, formally defined as
$\mathrm{Coverage} = \frac{\mathrm{Analysed}}{\mathrm{Corpus size}}$.
The results are presented in table~\ref{table:coverage}. 

\begin{table}
    \begin{scriptsize}
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        Corpus:            & \bf Gutenberg & \bf Wiki & \bf JRC acquis & \bf FTB \\
        \hline
        Coverage (tokens): & 96.4~\%       & 91.7~\%  & 93.4~\%        & 96.5~\% \\
        Coverage (types):  & 72.2~\%       & 60.8~\%  & 59.4~\%        & 72.2~\% \\
        \hline
    \end{tabular}
    \caption{Naive coverages when analysing common corpora
    \label{table:coverage}}
  \end{scriptsize}
\end{table}

Faithfulness is measured as a proportion of matched results in all results, or
formally $\mathrm{Faithfulness} = \frac{\mathrm{Matched}}{\mathrm{Correct} +
\mathrm{Missing}}$. In table~\ref{table:quality} we show the results first
by proportion of all tokens in data then by unique tokens.

\begin{table}
    \begin{scriptsize}
    \centering
    \begin{tabular}{|l|r|r|}
        \hline
        \bf Corpus & \bf Faithfulness \\
        \hline
        FTB (tokens) & 90.1~\% \\
        FTB (types)  & 48.0~\% \\
        \hline
    \end{tabular}
    \caption{Quality of the analyser measured as equality to results of
        another analyser \label{table:quality}}
  \end{scriptsize}
\end{table}

The sizes and processing speeds for automata built from the data described in
table~\ref{table:lexical} using Debian packaged HFST software
version~3.8.1\footnote{\url{http://wiki.apertium.org/wiki/Prerequisites_for_Debian}}
on a Dell XPS 13 laptop are given in table~\ref{table:size-speed}. The speed
was averaged over three runs using 1 million first tokens from europarl.
%the
%words per seconds rate is calculated using the formula
%$\frac{\mathrm{words}}{\mathrm{time}}$.

\begin{table}
    \begin{scriptsize}
        \centering
        \begin{tabular}{|l|r|}
            \hline
            \bf Feature & \bf Value \\
            \hline
            Size & 52M \\
            Speed & 955 wps\\
            \hline
        \end{tabular}
        \caption{Size of omorfi analyser as measured by \texttt{ls -lh}, speed
        of analysis using hfst-lookup in words per second \label{table:size-speed}}
    \end{scriptsize}
\end{table}

While this rudimentary check does not corroborate the results
of~\citet{silfverberg2009hfst} it is expected that something of that range is
doable with this methodology with some work.

\section{Discussion and Future Work}
\label{sec:discussion}

The coverage of the analyser is systematically around 98~\%, this is virtually
at the upper limits of reasonable results with the given corpora. This can be
noticed by analysing the errors or the out-of-vocabulary word-forms left in the
current corpora. For Wikipedia, we get word-forms like: \emph{Lä}, \emph{of},
\emph{The}, \emph{amp}, \emph{HMS}, \emph{and}, \emph{jpg}, that is, mainly
code notations, foreign words and abbreviations. In the Gutenberg corpus, we
get, among some missing proper nouns, forms like: \emph{nämät}, \emph{kauvan},
\emph{sitte} and other dialectal, spoken, archaic or otherwise non-standard
forms. Of course these are all plausible additions for morphological analyser
as well. Since this is the long tail part of the data, while
collecting and classifying further lexemes may be done it will require
considerable amount of work for little improvement, if there are applications
requiring higher coverage, devising guessers should be  considered instead.

The measurements of faithfulness we used in the evaluation section are based on
a ``gold standard'' which is merely a result of another automatic analyser.
This is not necessarily a representative of correctly analysed Finnish and thus
we have not regarded getting maximal faithfulness as a priority. As an example
of mismatched analyses right now: top wrong word-forms \emph{oli} `was',
\emph{olivat} `were' are analysed as present tense in the gold standard, we
feel this is incorrect and have opted not to match that analyses. A large part
of mismatches comes from analysis of compounds and derivations, whether they
are lexicalised or still treated as combinations of the root words, this can be
fixed by developing the lexicon.  Also, if one wishes to obtain 100~\%
faithfulness of FTB 3.1, the easiest path is to compile all the analyses from
the corpus into secondary automaton and use that along omorfi, although it is
unclear what the use case would be, beyond reproducing the mistakes contained
within that corpus.

One of the large parts of development in omorfi is the gathering and
classification of lexical data based on our classification scheme and ad hoc
longest suffix guessers in supervised lexicon development environment, which
has been suitable for gathering nearly 300,000 lexemes in scope of
past 4 years. It would be interesting to know how more advanced machine learning
methods had improved this.

Related work in machine learning of morphology of Finnish, such
as~\cite{durrett2013supervised}\footnote{We thank the anonymous reviewer for
bringing this recent research to our attention} shows recall rates of around
83---87~\% for predicting inflection of Finnish words, however, their goal was
to learn to predict Wiktionary's example inflection table's 28 forms per noun
and 53 forms per verb, and they only performed intrinsic evaluation on held-out
Wiktionary pages, it is hard to directly compare how this relates to our system
capable of analysing and generating all of approximately\footnote{subject to
    allomorphic variations and some semantic restrictions: \emph{omena} `apple'
    has different amount of forms than \emph{talo} `house' and \emph{tuulla}
`wind' v. has different amount of forms than \emph{tulla} `come'.} 5,268 forms
per noun and 147,618 (full participle paradigms subsumed) forms per verb plus
an infinite amount compounded combinations and derivations of those.

It is noteworthy that in an open source project with multiple active
developers, importance of the evaluations is somewhat diminished by the
relatively active development pace. That is, if there are some missing analyses
or low score is a problem, it will likely be improved or fixed in a short
time-frame. For example, the faithfulness measure has been varying in range of
80---93~\% during the preparations of this article.

In the future it would be interesting to see some development on an end-user
application that necessitates a use of high-quality disambiguated morphological
analyses. This would ultimately entail the use of more advanced statistical methods, and
better training data to go with the methodology, as the current system only
contains beginnings of statistical or rule-based disambiguation, that was
necessary for applications such as spell-checking.  Having such application would
not only be crucial to developing better methods for disambiguation but also in
creating good extrinsic evaluation criteria to show real effects of system's
morphological analysis capabilities beyond measurement of faithful reproduction
of bugs of an unknown closed-source system's bugs.

\section{Conclusion}
\label{sec:conclusion}

In this squib we set out to present a new fully open source Finnish
morphological lexicon. We confirm that it is full-fledged and mature lexical
database that can be used as a baseline morphological analyser with large
coverage, suitable for linguistic research, as well as in external applications such as spelling correction and rule-based machine translation. We have shown
some approaches that make available use of modern natural language processing
techniques like statistics in conjunction with analysers built from our data
and paved a way forward for researchers interested those topics. We also provide
some easy-to-access ways for linguists and researchers to use and extend our
database via publicly maintained servers and crowd-sourced web-based services.

% apalike with underscores???
\bibliographystyle{apalike}
\bibliography{skyomorfi2015}
\iffinal
\newpage
\begin{small}
    \noindent Contact Information:\\
\\
    Tommi A Pirinen\\
    Ollscoil Chathair Bhaile Átha Cliath\\
    Baile Átha Cliath 9\\
    Eire\\
    e-mail: \url{Tommi.Pirinen@computing.dcu.ie}\\
\end{small}
\fi
\end{document}
% vim: set spell:
