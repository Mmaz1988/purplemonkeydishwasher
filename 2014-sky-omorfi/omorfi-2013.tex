\documentclass[a4paper,12pt]{article}

\newif\iffinal
\finalfalse

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{expex}

\usepackage{url}
\usepackage{hyperref}

\usepackage[margin=3cm]{geometry}

\usepackage{natbib}
\setlength{\bibsep}{0pt}
\linespread{1.3}

\defaultfontfeatures{Mapping=tex-text}
\setmainfont{Times New Roman}

\title{Omorfi---Open-Source Weighted Finite-State Morphological Analysis of
Finnish}
\iffinal
\author{Tommi A Pirinen \\
    Dublin City University\\
    Centre of Next Gen. Localisation}
\fi
\date{}
\begin{document}

\maketitle
\begin{abstract}

    This squib describes a contemporary system for the computational analysis of
    the morphology of Finnish word-forms. The system we present is at its core a
    finite-state morphology, just like other popular systems for computational
    Finnish morphology have been since 1983.  The purpose of this squib is to
    describe the key differences against the earlier analysers.  We have used
    some of the recent ideas from the field of statistical computational
    linguistics to create the first full-fledged hybrid rule-based and
    statistical finite-state system for morphological analysis.

    Our system is based fully on free and open source data and methods. This
    brings in focus some new approaches to the lexicographical data collection
    using crowd\-sour\-ced dictionary Wiktionary, as well as statistical
    component from the Wikipedia data.

    To evaluate our analyser, we have processed large text corpora with it
    and verified that only 1~\% of the word-forms remains unknown to the
    system. We also show that omorfi can be tuned to match the FinnTreeBank 3.1
    standard at 85~\% faithfulness for interoperability.
\end{abstract}
\section{Introduction}

Computational morphological analysis is a central component for most of the
computational applications of linguistic analysis. Computational morphological
analysis of Finnish language was first described some 30 years
ago~\citep{koskenniemi1983twolevel}. The aim of this paper is to present the
newest approach to the task and examine the differences of our current approach
to traditional systems, and to highlight some of the new development. The
new development that we discuss is mainly in line with the recent trends in
the field of computational linguistics.

One of the main goals of this paper is to act as the central scientific
documentation of our current morphological analyser, and we have made an
attempt to highlight the long-term design goals of the system instead of
transitional and volatile features of a fast-moving computer software that is
developed by a base of open-source and language enthusiasts. On the other hand,
this is also a scientific squib, so we describe shortly the most recent
modifications made to the hybrid rule-based and statistical model of Finnish
language on, and the additions in the dictionary data model we made to
accommodate the semi-automatic crowd-sourced lexicography using Wiktionary as a
lexical data source.  We also refer to further use of omorfi by numerous
researchers for various tasks, such as
spell-checking~\citep{pirinen2014weighted}, language
generation~\citep{toivanen2012corpus} and statistical
models~\citep{haverinen2013building,bohnet2013joint}. We consider this
as additional evidence of usability of hybrid statistical and rule-based
approaches for weighted finite-state morphological analysis.

The statistical approach to language modeling has been popular throughout
2000's, however, with morphologically complex languages it brings additional
challenges since data is scarce and requirement for word-forms to get
discriminative probabilities is higher.\footnote{We assume here that
morphological complexity necessarily means more word-forms per word, while this
may not always be the case, it is true for Finnish} In omorfi we have used a
hybrid approach where the morphological analyser is defined in the traditional
manner using inflection paradigm classified dictionary and rules in style
of~\citet{koskenniemi1983twolevel}, and the result is trained using frequencies
of word-forms found in the corpus. We show some combinations of contemporary
methods of combining the rule-based analysers with statistical data, such as
ones described by~\citet{pirinen2009weighted} and~\citet{pirinen2012improving}.
In fact, one of the main contributions of the system is the possibility to pick
and mix combinations of statistical methods used in contemporary research. That
is, in this article we do not go into details of optimising parameters but
rather present a system that has parameter tuning capabilities.


Another recent development in the computational language models is the concept
of \emph{maintainability} of these computational systems, e.g.
by~\cite{maxwell2008joint,pirinen2011modularisation}. Specifically we will show
how we use the power of \emph{crowd-sourcing} to keep up with the new words,
neologisms and other rare words missing from dictionaries. In particular we
study the use of the popular online dictionary Wiktionary as a source of
additional lexical data. To support semi-automatic use of Wiktionary data we
have devised a morph-based formulation of the finite-state morphology of
Finnish which we test in conjunction with basic classification schemes to help
users to classify words.

One notable practical distinction in our system is its licensing policy. Omorfi
analyser is a free and open source product.\footnote{
\url{http://code.google.com/p/omorfi/}}  In contemporary computational
linguistics, freeness of systems and data is rightly seen as a cornerstone of
properly conducted science, as it fulfills the requirement of repeatability by
not setting unnecessary fences for the repetition of the scientific results.
There is a large base of recent research supporting this, specifically for
Finnish the latest is by~\citet{koskenniemi2008build}. For computer-savvy end
users, such as language technologists, this means that the tools necessary to
perform linguistic analysis with omorfi can be downloaded to any average PC,
and for the rest of the scientists we co-maintain a usable installation at
CSC.\footnote{\url{http://www.csc.fi/english/research/sciences/linguistics}, 
for universities outside Finland you may be able to convince your local data
center to install accessible version.}

To summarise, this article is an overview of a new Finnish morphological
analyser.  It consists of the methodological advances found from past years
research of finite-state morphologies and computational linguistics, and
contrasts the resulting system to the previous state-of-the-art in
morphological analysis of Finnish. It also shows some recent uses of a
morphological analyser in real world applications. We also show how
crowd-sourced lexicography can be used as a part of the system.

\section{Methods}
\label{sec:methods}

There have been numerous implementations of computational morphological
analysis of Finnish along the years. These  can be dated back to at least
1980's, including~\citet{koskenniemi1983twolevel}, which is among the more
influential works in the field of computational morphology. 
Otherwise, the implementation of our analyser
follows the traditional works on finite-state morphology
by~\citet{beesley2003finite}. On top of that we
have applied recent extensions from the research of finite-state morphology,
such as weighted finite-state methods~\citep{openfst,hfst2012}. This brings the
traditional rule-based language analyser towards the statistical language
analysers that are widely popular in the handling of morphologically less
complex languages.

A rule-based analyser is implemented based on the classification of the words
by their inflectional patterns. The patterns in Finnish are based on two parts:
stem variations and suffix allomorphs. The classified dictionary words are
stripped of their varying stem parts, and then concatenated with the variations
and then stems by means of standard finite-state morphology. E.g. dictionary
word \emph{vesi} `water' has stem variation in \emph{-si} $\sim$ \emph{-te-}
$\sim$ \emph{-de-} $\sim$ \emph{-t-}, and respectively suffixes $\emptyset$
(nominative) $\sim$ \emph{-nä} (essive, `as water') $\sim$ \emph{-ssä}
(inessive `in water')  $\sim$ \emph{-tä} (partitive, roughly `of water'), which
is arranged in morpheme sublexicons as in \emph{Finite State
Morphology}~\citep{beesley2003finite}. Shortly said, such finite-state
formulation of morphotactics is a set of disjunctions of paths in the automaton
representing the morphs. The morphs are extended by their classifications, and
the morphotactic rules over classifications are enforced using composition
operation, such as: noun root morphs are followed by case morphs and then
optionally possessives and clitics~\citep{hfst2012}.

A common way to apply statistical methods for morphological analysis is
straight-forward; to get the likelihood $P(w)$ for the surface form $w$, we
calculate the amount of word-forms $f(w)$ in a corpus and divide it by the
number of word-forms in the whole corpus $CS$: $P(w) = \frac{f(w)}{CS}$.  To
get around the problems with the probability of $0$ for unseen word-forms, we
use additive smoothing~\citep{chen1999empirical}, which estimates likelihood of
each type as $1$ larger than it is and size of corpus as number of types
larger. The acquired likelihoods can be combined to the rule-based
morphological analyser using basic finite-state operations such as composition
of the weighted disjuncted word-form paths.

Another method from the recent research on training of the compounding language
models~\citep{pirinen2009weighting} we use is the estimation of the compound
words by their parts. This means that given unseen compound \emph{banaaniovi},
the estimated likelihood is $P(\mathrm{banaani}) \times P(\mathrm{ovi})$,
rather than $P(\mathrm{banaaniovi})$. One of the non-apparent results of such
training is that in case of ambiguous compounds, the analyser will
systematically prefer the ones with least words.

To help the collection of new lexical data, we have used some automatic
classification methods, such as longest common suffix search from the
morphological dictionary.  The way it works is that given a new word-form, it
can go through the dictionary to find a longest suffix it has in common with
some existing word-form. For example  given a new word-form \emph{googlella}
`with google', the system can guess that it might be like \emph{nallella} `with
a teddy bear', or \emph{puolella} `with a half' (common suffix of 5
characters), and using the classification then suggest the end user or corpus
harvester to study forms like: \emph{googlen, googlea, googleen, googlet,
googlejen, \ldots} versus \emph{googlen, ${}^\star$googlta, googleen, googlet,
${}^\star$googlten, \ldots} to decide on the classification of the new word. To
help this semi-automatic classification we have reformulated morpheme-based and
phonologically expanded finite-state morphologies as plain morph-based
morphology.

\section{Data}
\label{sec:data}

There are a few freely available open resources for lexicographical data of
Finnish. The first one  we used is based on a dictionary maintained by Research
institute of languages in Finland (RILF). The lexicographical data of the
dictionary has been available under free software licence LGPL since 2007 as
\emph{Nykysuomen
sanalista.}\footnote{\url{http://kaino.kotus.fi/sanat/nykysuomi}}.  The second
source of lexical data we acquired from the internet is a free, open source
database named
\emph{Joukahainen.}\footnote{\url{http://joukahainen.puimula.org}}  For another
source of lexical data we use the popular crowd-sourced \emph{Wiktionary}
project.  Finally, we have used data that has been produced for various
projects at university of Helsinki, including other lexical projects, such as
FinnWordNet~\citep{linden2010finnwordnet}.\footnote{The primary sources for
lexical data used in these projects include europarl corpus and English
wordnet, but not any of the corpora we have used for evaluations: Wikipedia,
JRC acquis or Gutenberg.} The classification scheme and precision between
these sources vary, so the semi-automatic classification method specified in
section~\ref{sec:methods} has been useful, as evidenced first
by~\citet{listenmaa2009combining} and numerous contributors thereafter.

The full lexical data used in the current version of our analyser consists of
396,673 classified lexemes, the classification is roughly summarised by word's
classes and their origin in table~\ref{table:lexical}. 

\begin{table}
    \begin{scriptsize}
  \centering
    \begin{tabular}{|l|r|r|r|r||r|}
        \hline
        \bf Database: & Kotus & Joukahainen & Wiktionary & UH & \bf Total \\
        \bf Class   & & & & & \\
        \hline
        Adjectives     & 10,537 & 652   & 59    & 6,368   & 17,616  \\
        Nouns          & 67,608 & 2,945 & 1,133 & 1,589   & 73,275  \\
        (Proper nouns) & 20     & 5,394 & 70    & 262,330 & 267,814 \\
        Verbs          & 9,685  & 476   & 12    & 499     & 10,672  \\
        Others         & 6620   & 5     & 12    & 25,536  & 32,173  \\
        \hline
        \bf Total      & 94,290 & 9,472 & 1,286 & 296,322 & 396,637 \\
        \hline
    \end{tabular}
  \caption{Lexical data used in the analyser.  The columns are the lexical
      sources used: Nykysuomen sanalista in column titled \emph{Kotus} (RILF),
      \emph{Joukahainen}, \emph{Wiktionary}, terms harvested in various
      research project in University of Helsinki in column titled \emph{UH},
      and a total sum in the last column. The rows represent coarse
      morphological classification, with proper nouns separated into their own
      row. On the \emph{others} row are the adverbs, adpositions and such words
      with defective paradigms. These words in current dictionaries have one
      entry per inflectional ending.
  \label{table:lexical}}
  \end{scriptsize}
\end{table}

For measurements we use only freely available corpora. The size of these is
detailed in table~\ref{table:corpora}. The corpora are following: ebooks of project
Gutenberg\footnote{\url{http://gutenberg.org}}, the data of Finnish
Wikipedia\footnote{\url{http://fi.wikipedia.org}}, and the JRC acquis
corpus\footnote{\url{http://ipsc.jrc.ec.europa.eu/index.php?id=198}}. For
downloading and preprocessing these corpora we use freely available
scripts~\footnote{\url{https://github.com/flammie/bash-corpora}}. The scripts
retain most of the punctuation and white-space without touching. The resulting
token counts are given in~\ref{table:corpora}. Some further tests were made
with fully tokenised and analysed
FinnTreeBank~\citep{voutilainen2012specifying} version 3.1.  The scripts used
for evaluation are part of omorfi source code.

\begin{table}
    \begin{scriptsize}
  \centering
    \begin{tabular}{|l|r|r|}
        \hline
        \bf Feature: & Tokens     & Types     \\
        \bf Corpus   &            &           \\
        \hline
        Gutenberg    & 36,743,872 & 1,590,642 \\
        Wikipedia    & 55,435,341 & 3,223,985 \\
        JRC Acquis   & 42,265,615 & 1,425,532 \\
        FTB          & 76,369,439 & 1,648,420 \\
        \hline
    \end{tabular}
  \caption{Corpora used for evaluations. Tokens are all strings extracted from
      corpus and types are unique strings, both include punctuation and some
      codified expressions like URLs, addresses etc.
  \label{table:corpora}}
  \end{scriptsize}
\end{table}

\section{Evaluation}
\label{sec:evaluation}

In this section we evaluate our analyser using very basic methods of
evaluation and free open source data. Firstly, we measure a naive coverage by
trying to analyse large corpora and see the proportion of word-forms that can
be analysed at all. Then we measure the quality of analysis as compared to
reference corpora of morphologically analysed and disambiguated texts. To
match omorfi analyses with other analyser's output we used a python script
that basically does naive string matching. The version of omorfi we used
was 
\verb|a1321fab6a80|\footnote{\url{https://code.google.com/p/omorfi/source/detail?r=a1321fab6a80}} 
configured with default settings.


First we measure how big proportion of data in the material are
out-of-vocabulary items. This gives us naive coverage, formally defined as
$\mathrm{Coverage} = \frac{\mathrm{Analysed}}{\mathrm{Corpus size}}$.
The results are presented in table~\ref{table:coverage}. 

\begin{table}
    \begin{scriptsize}
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        Corpus:            & \bf Gutenberg & \bf Wiki & \bf JRC acquis & \bf FTB \\
        \hline
        Coverage (tokens): & 98.2~\%       & 97.6~\%  & 98.6~\%        & 99.4~\% \\
        Coverage (types):  & 61.1~\%       & 59.0~\%  & 55.5~\%        & 70.8~\% \\
        \hline
    \end{tabular}
    \caption{Naive coverages when analysing common corpora
    \label{table:coverage}}
  \end{scriptsize}
\end{table}

Faithfulness is measured as a proportion of correct results in all results, or
formally $\mathrm{Faithfulness} = \frac{\mathrm{Correct}}{\mathrm{Correct} +
\mathrm{Missing}}$. In table~\ref{table:quality} we show the results first
by proportion of all tokens in data then by unique tokens.

\begin{table}
    \begin{scriptsize}
    \centering
    \begin{tabular}{|l|r|r|}
        \hline
        \bf Corpus & \bf Faithfulness \\
        \hline
        FTB (tokens) & 83.6~\% \\
        FTB (types)  & 48.0~\% \\
        \hline
    \end{tabular}
    \caption{Quality of the analyser measured as equality to results of
        another analyser \label{table:quality}}
  \end{scriptsize}
\end{table}

\section{Discussion and Future Work}
\label{sec:discussion}

Our weighted finite-state analyser combines rule-based approach with statistics
and is currently capable of basic morphological disambiguation.  For future
research, it should be interesting to see more advanced statistical methods
applied as well as combinations with other rule-based systems such as
constraint grammar~\citep{karlsson1995constraint}.

To get the lexical data organised and the analyser implemented we have extended
the lexical classification using a simple morph-based approach to synchronic
morphophonology.  One of the implications of the systematic classification
schemes is that it allows for implementations based on poorer computational
morphology implementations to be made. This means that the lexical data could
be used in systems without morphophonology support, such as
hunspell~\citep{tron2005hunmorph} or apertium~\citep{apertium}, which are
popular enough to be considered the de facto standard systems for computational
linguistics in fields of spell checking and machine translation respectively.
It would be greatly useful to see the system formulated under these formalisms.

The coverage of the analyser is systematically around 98~\%, this is virtually
at the upper limits of reasonable results with the given corpora. This can be
noticed by analysing the errors or the out-of-vocabulary word-forms left in the
current corpora. For Wikipedia, we get word-forms like: \emph{Lä}, \emph{of},
\emph{The}, \emph{amp}, \emph{HMS}, \emph{and}, \emph{jpg}, that is, mainly
code notations, foreign words and abbreviations. In the Gutenberg corpus, we
get, among some missing proper nouns, forms like: \emph{nämät}, \emph{kauvan},
\emph{sitte} and other dialectal, spoken, archaic or otherwise non-standard
forms. Of course these are all plausible additions for morphological analyser
as well.

The measurements of faithfulness we used in the evaluation section are based on
a ``gold standard'' which is merely a result of another automatic analyser.
This is not necessarily a representative of correctly analysed Finnish and thus
we have not regarded getting maximal faithfulness as a priority. As an example
of mismatched analyses right now: top wrong word-forms \emph{oli} `was',
\emph{olivat} `were' are analysed as present tense in the gold standard, we
feel this is incorrect and have opted not to match that analyses. A large part
of mismatches comes from analysis of compounds and derivations, whether they
are lexicalised or still treated as combinations of the root words. Finally,
minor differences in participle analyses may account for around
fifth\footnote{This was estimated by counting all participles in subset of
errors and sampling manually, some of these may be different errors
nevertheless.} of the analysis differences by token count at the moment (that
is, FTB considering participle as a part of speech with root being an inflected
form of verb where omorfi includes it in verbal analyses with verb infinitive
as root).

It is noteworthy that in an open source project with multiple active
developers, importance of the evaluations is somewhat diminished by the
relatively active development pace. That is, if there are some missing analyses
or low score is a problem, it will likely be improved or fixed in a short
time-frame. For example, the faithfulness measure has been varying in range of
80---93̃~\% during the preparations of this article.

\section{Conclusion}
\label{sec:conclusion}

In this article we set out to show some features of the newest edition of our
Finnish morphological analyser. We confirm that it is full-fledged analyser
system by measuring coverage of multiple texts of different types and from
different times.  We studied the usefulness of systematic re-classification of
Finnish inflectional paradigms based on synchronic orthographical evidence, and
found that it provides some benefits for tasks like automatic classification
and verification of new lexical data.

% apalike with underscores???
\bibliographystyle{apalike}
\bibliography{omorfi2013}
\iffinal
\newpage
\begin{small}
    \noindent Contact Information:\\
\\
    Tommi A Pirinen\\
    Dublin City University\\
    Glasnevin\\
    Dublin 9\\
    Ireland\\
    e-mail: \url{Tommi.Pirinen@computing.dcu.ie}\\
\end{small}
\fi
\end{document}
% vim: set spell:
