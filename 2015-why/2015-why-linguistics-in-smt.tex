\documentclass[a4paper]{article}
%\documentclass[preprint]{flammie}

\usepackage{fontspec}
\usepackage{url}
\usepackage{hyperref}
\setmainfont{Liberation Serif}

\title{Role of Linguistics in Improving Statistical Machine Translation as
Scientific Field and as End-Result}

\iffalse
\author{Tommi A Pirinen\\
Dublin City University\\
ADAPT Centre-School of Computing\\
\url{tommi.pirinen@computing.dcu.ie}}
\date{Last Modification: \today}
\fi

\begin{document}

\maketitle

\begin{abstract} 
    In statistical machine translation (SMT), the main goal behind
    most of the research work to date is to improve translation quality as
    measured by automatic evaluation metrics (e.g. BLEU, METEOR, TER). These
    metrics are cheap as they are fully automatic and they are useful in that
    they (are supposed to) correlate with human translation evaluation when
    measured on large bodies of professionally translated texts. That said, a
    mere race for tiny improvements in these metrics (i.e. there is a common
    type of paper in the field  that (i) adds some novel functionality to the
    overall SMT pipeline and (ii) reports a statistically significant
    improvement in terms of BLEU as evidence of its usefulness, without
    additional human evaluation nor in-depth analysis) has not been overly
    successful with all of the SMT language pairs.

    This seems to be especially the case for some of the non Indo-European
    languages, e.g. Finnish or Turkish.  An example of this is reflected in the
    state-of-the-art of machine translation for the Finnishâ€“English language
    pair, which has not advanced in the past decade in terms of these metrics.
    These metrics however, are based on simple string comparisons and
    substitutions, and as it is argued in the papers published on this language
    pair that show little to no
    improvement\cite{clifton2011combining,luong2010hybrid,virpioja2007morphology},
    these metrics may not be ideal for a morphologically complex language like
    Uralic or Turkic ones.

    We believe that using metrics that include linguistically-motivated
    measures (e.g. MEANT\cite{lo2011meant}) would be more appropriate. For example, a simple
    compounding mistake or a wrong allomorph for case suffix is penalised by
    BLEU (basically, a word matches or does not match the reference) while the
    fluency and meaning in the translation is mostly retained.

    A possible way forward could lie in the combination of metrics that combine
    matching at word and sub-word levels. In this regard, taking the most
    widely-used automatic metric, BLEU, we suggest to combine its original
    implementations (i.e. word level) with its proposed implementation at
    morpheme level (m-BLEU\cite{luong2010hybrid}) level. At the very least,
    such a metric should shed light on the conspicuous lack of advance in SMT
    for this language pair.

    Apart from the issue of non-linguistic evaluation metrics, a meta-analysis
    of the state-of-the-art in Finnish-English SMT that we have conducted
    identifies two parts of the process that would greatly benefit from
    introducing a linguistic view, namely (i) the initial hypothesis when
    devising a new SMT system and (ii) the final error analysis of the MT
    output. In fact, in most of the papers (i) the experimental set-up is not
    motivated by any relevant linguistic means and (ii) error analysis is not
    conducted at all, let alone in linguistic detail, which would be necessary
    to design future work aimed at improving current results in terms of
    translation quality. 
\end{abstract}


\bibliographystyle{unsrt}
\bibliography{why2015}

\end{document}
