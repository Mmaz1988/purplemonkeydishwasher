\documentclass[officiallayout]{unihelcompling}
%\documentclass[officiallayout,draft]{unihelcompling}

\usepackage{amssymb}

% hacking citations
\usepackage{natbib}

\usepackage{polyglossia}

% for "= hacks
\usepackage[shortcuts]{extdash}

\setdefaultlanguage{english}
\setotherlanguages{finnish,russian}

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}

\usepackage{url}
\usepackage{hyperref}
\usepackage[obeyDraft]{todonotes}


\usepackage{multirow}

\usepackage[normalem]{ulem}


% hacking the images
\usepackage{graphicx}
\usepackage{geometry}

\newcommand\misspelt{\bgroup\markoverwith
{\textcolor{red}{\lower3.5pt\hbox{\sixly \char58}}}\ULon}

\setmainfont[Mapping=tex-text]{Times New Roman}

\title{Weighted Finite-State Methods for 
\misspelt{
Spell-Checking
}
and Correction\footnote{Summer draft}}
\author{Tommi A Pirinen}


\authorcontact{\url{tommi.pirinen@helsinki.fi}\par
  \url{http://www.helsinki.fi/\%7etapirine}}
\pubtime{Oddmonth}{2013}
\reportno{0}
\isbnpaperback{000-00-0000-0}
\isbnpdf{000-00-0000-0}
\issn{0000-0000}
\printhouse{Unigrafia?}
\pubpages{000} % --- remember to update this!
\supervisorlist{Krister Lindén, University of Helsinki, Finland}
\preexaminera{---}
\preexaminerb{---}
\opponent{---}
\custos{Anssi Yli-Jyrä, University of Helsinki, Finland}
\generalterms{thesis, finite-state, spell-checking, language model,
  morphology}
\additionalkeywords{statistical language models, morphologically complex
languages}
\crcshort{F.1.1, I.2.7, I.7.1}
\crclong{
\item[F.1.1] Finite-State Automata
\item[I.3.1] Natural Language Processing
\item[I.7.1] Spelling
}
\permissionnotice{
  To be presented in \ldots{} text of a long permission notice. Text of
  a long permission notice. Text of a long permission notice. Text of
  a long permission notice. Text of a long permission notice. Text of
  a long permission notice.
}



\date{\today}

\begin{document}

\frontmatter

\maketitle

\begin{abstract} 
    This dissertation is a large-scale study of spell\-/checking and correction
    using finite\-/state technology. Finite-state spell\-/checking is a key method
    for handling morphologically complex languages in a computationally
    efficient manner. The dissertation goes through technological and practical
    considerations that are required for finite\-/state spell-checkers to be at
    the same level as state-of-the-art non-finite\-/state spell-checkers. 

    Three aspects of spell\-/checking are considered in the thesis: modeling of
    the correctly written words and word-forms with finite\-/state language
    models, applying statistical information to finite\-/state language models
    with a specific view to morphologically complex languages, and modeling the
    misspellings and typing errors using finite\-/state automata-based error
    models.

    The usability of finite\-/state spell-checkers as a viable alternative to
    traditional software-based solutions is shown by a large-scale
    evaluation of speed and quality of spell\-/checking with languages of
    a morphologically different nature. The selected languages display a
    full range of typological complexity from isolating English to 
    poly-agglutinative Greenlandic via Finnish and the Saami languages.
\end{abstract}

\mainmatter

\chapter*{Preface}
\label{chap:preface}

Spell-checkers are very basic-level, commonly used practical software.
Ubi\-quitous enough that nowadays they are in everyday use for most of us,
whether in office software suites, Facebook text fields or a mobile
phone autocorrect utility. The practical motivation for this thesis comes
first and foremost from the disappointment in contemporary applications for
smaller, more complex languages than English, like my native language, Finnish.
Why does not my telephone know \emph{my Finnish}? Even I could implement this
better! And so I did.

The scientific background of the thesis builds on the research of the
computational linguistics scene at the university of Helsinki, starting from my
master's thesis work, which was a finite\-/state model of the Finnish
language---using this language model as part of a practical piece of end-user
software, and towards advancement of scientific treatment of the problems
when doing so, it was a very obvious and motivating incentive for a doctoral
dissertation. While the hard science behind the thesis is, in my opinion,
delightfully simple, I believe the findings to be interesting for anyone
working in computational linguistics, or even natural language engineering,
especially when it concerns languages different from English, by complexity,
availability of resources, or otherwise.

The improvements to, and methods for, the spell\-/checking and correction
presented in the thesis should provide a good base for future more efficient
spell\-/checking, but they are not yet included popular end-user applications---
because I believe the productification and social engineering required to make
it happen is not part of the research work. The current approaches have been
tested on colleagues, local students, and a select few brave alpha-testers from
Greenland, whose native language is well-known to be trickier to handle than
most of ours. As the results of the testing were encouraging, we can hope that
the future brings better spell-checkers and autocorrections for us with
slightly more tricky native languages.

\section*{Acknowledgements}
\label{sec:acknowledgements}

Official thanks go to FIN-CLARIN project for funding my research, and for Yliopistotutkintolautakunta, for providing spelling error materials.
Besides that, I am not a person who explicitly thanks any one. It
should be noted though that if you are reading this text searching for your
name, it is the most likely that you deserve thanks; obviously keeping track of
everyone I owe gratitude for is not an easy task for disorganised person like
myself, so here is a randomly organised list of things I am thankful for, or
have been thankful for during the idle times of writing this thesis, in
absolutely no particular order and without doubt full of omissions.

The first obvious contribution for the whole thesis project and all comes from
the local research group and other colleagues in University of Helsinki. The
HFST project for working on the software and the FinnTreeBank and Finnish
Wordnet project for providing that extra bit of help with Finnish language
model building. Specifically I would like to thank my original master's thesis
advisor Kimmo Koskenniemi for leading me to this line of work, my PhD thesis
advisor and HFST project leader Krister Lindén for keeping the goal in sight
throughout the thesis project and helping managing the times and ideas.  In
HFST team I owe to Sam Hardwick, for doing really the majority of software work
in \texttt{hfstospell} branch of project, Miikka Silfverberg for providing the
statistical and context-based finite\-/state models that go beyond my expertise
and Erik Axelson for maintaining the whole software behemoth.  I am very
grateful to Per Langgård for picking my contribution in LREC 2011 up as a way
to get the next version of Greenlandic speller to people and thus introducing
me to whole intriguing problem field of poly-synthetic languages' computational
modelling. The thesis and its focus would be quite different without it.  I
thank Jack Rueter for numerous fruitful discussions on all topics related to
uralic languages, among others. I thank whole Divvun / Giellatekno project at
University of Tromsø for the computational morphologies which we have been
slowly turning into spell-checkers, especially Sjur Moshagen and Trond
Trosterud for co-operation in this project. And I also thank Apertium project
and contributors for another fine source of linguistic materials to turn into
spell-checkers.

On personal level I am thankful to my family, my friends in university of
Helsinki, especially those in student organisations like Aspekti and Hyrmy, and
the one deity who shares the initials with finite\-/state machines,
FSM---without them, none of this would have been possible.

\textfinnish{Helsingissä \today,\\
---T.~F.~A.~P.}

\chapter*{Original Papers}
\label{chap:papers}

This thesis consists of an introduction and the following peer-reviewed publi-
cations, which are referred to as Papers I–X in the text. These publications
are reproduced at the end of the print version of the thesis.

\defcitealias{pirinen2009weighted}{I}
\defcitealias{pirinen2009weighting}{II}
\defcitealias{pirinen2010finitestate}{III}
\defcitealias{pirinen2010building}{IV}
\defcitealias{pirinen2010creating}{V}
\defcitealias{pirinen2011modularisation}{VI}
\defcitealias{pirinen2012compiling}{VII}
\defcitealias{pirinen2012improving}{VIII}
\defcitealias{pirinen2012effects}{IX}
\defcitealias{pirinen2013quality}{X}

\begin{enumerate}
    \item[\citetalias{pirinen2009weighted}]
        Krister Lindén and Tommi Pirinen.
        \newblock Weighted finite-state morphological analysis of Finnish 
        compounds.
        \newblock In \emph{Nodalida}, Odense, Denmark, 2009.
    \item[\citetalias{pirinen2009weighting}]
        Krister Lindén and Tommi Pirinen.
        \newblock Weighting finite-state morphological analyzers using HFST
        tools.
        \newblock In \emph{Finite-State Methods in Natural Language Processing},
        Pretoria, South-Africa, 2009.
    \item[\citetalias{pirinen2010finitestate}]
        Tommi~A Pirinen and Krister Lindén.
        \newblock Finite-state spell-checking with weighted language and error
        models.
        \newblock In {\em Proceedings of the Seventh SaLTMiL workshop on 
            creation and use of basic lexical resources for less-resourced 
        languagages}, Valletta, Malta, 2010.
    \item[\citetalias{pirinen2010building}]
        Tommi~A Pirinen and Krister Lindén.
        \newblock Building and using existing Hunspell dictionaries and {\TeX }
        hyphenators as finite-state automata.
        \newblock In {\em Proccedings of Computational Linguistics -
            Applications, 2010}, Wis{\l}a, Poland, 2010.
    \item[\citetalias{pirinen2010creating}]
        Tommi~A Pirinen and Krister Lindén.
        \newblock Creating and weighting Hunspell dictionaries as finite-state
        automata.
        \newblock {\em Investigationes Linguistic\ae}, 21, 2010.
    \item[\citetalias{pirinen2011modularisation}]
        Tommi~A Pirinen.
        \newblock Modularisation of Finnish finite-state language 
        description—towards wide collaboration in open source development of
        morphological analyser.
        \newblock In {\em Proceedings of Nodalida}, volume~18 of {\em NEALT
        proceedings}, Rīga, Latvia, 2011.
    \item[\citetalias{pirinen2012compiling}]
        Tommi A Pirinen and Francis M. Tyers.
        \newblock Compiling apertium morphological dictionaries with HFST and
        using them in HFST applications.
        \newblock {\em Language Technology for Normalisation of Less-Resourced
        Languages}, 2012.
    \item[\citetalias{pirinen2012improving}]
        Tommi A Pirinen, Miikka Silfverberg, and Krister Lindén.
        \newblock Improving finite-state spell-checker suggestions with part of
        speech n-grams.
        \newblock In {\em CICLING}, Delhi, India, 2012.
    \item[\citetalias{pirinen2012effects}]
        Tommi~A Pirinen and Sam Hardwick.
        \newblock Effects of weighted finite-state language and error models on
        speed and efficiency of finite-state spell-checking.
        \newblock In {\em FSMNLP}, Donostia--San Sebastían, Spain, 2012.
    \item[\citetalias{pirinen2013quality}]
        Tommi A Pirinen.
        \newblock Quality and Speed Trade-Offs in
        Finite-State Spell-Checking and Correction, \misspelt{forthcoming}
\end{enumerate}

The implementations and data for reproducing of the results is available
in the version control system of the HFST project\footnote{\url{http://svn.code.sf.net/p/hfst/code/trunk/articles/}}.

\tableofcontents

\listoftables

\listoffigures

\listoftodos

\chapter{Introduction}
\label{chap:introduction}

Spell-checking and correction is among the more practical, well-understood
subjects in computational linguistics, and earlier, computer science.
The task of detecting spelling mistakes in different texts---written, scanned,
or otherwise---is a very simple concept to grasp. And the computational
handling of the problem has been the subject of research for almost as long as
computers have been capable of processing texts, with first influential
academic works published in the 1960's, and the field has greatly developed
since.

The purpose of this thesis is to research one specific approach to
spell\-/checking---that of finite\-/state technology. The finite-state technology
has its roots in the mathematical theory of formal \emph{languages}, which I
will refer to as \emph{string sets} throughout this thesis to avoid confusion
with the more common meaning of the term, natural languages. The theoretical
beauty of this approach will be recapped more closely in the later subsections.
The practical circumstances of this approach are the following: it gained
popularity among computational linguists interested in morphologically complex
languages around the 1980's and onwards, and is thought by some of us to be the
only way to handle more morphologically complex languages. This sets the
direction of this thesis to \emph{implement spell\-/checking for languages of
varying morphological complexity} efficiently.

Finite-state approaches are traditional to computational handling of Finnish,
cf.~\citet{koskenniemi1983twolevel}. Finnish is my native language and for
practical reasons a recurring theme in this thesis. Whilst concentrating on
scientific contributions made during the years, this thesis is in a way also a
book describing the past years of development of the actual, language
independent, spell\-/checking software\footnote{available, free and open source,
like all good scientific research products} which is used also in making of
this very thesis. So, it should be clear that the contribution of this thesis
is meant to be a very concrete, should I even say a final---albeit
beta-quality---system for spell\-/checking.  There are some practical
implications of using one's native language to evaluate and develop new and
improved methods; the motivation for spending immeasurable amounts of spare
time is there.

One common theme throughout the thesis is efficiency. As we are working with a
practical system with potential everyday use, we need to ground the theoretical
advances so that they are practically usable. For this reason the systems used
in the thesis are also mainly limited to freely available and open source
materials and techniques. Indeed, if a spell\-/checking sub-system is mentioned
in this thesis, you should at least be able to download a functional prototype.
All our software and data is available in our version management
system\footnote{\url{http://svn.code.sf.net/p/hfst}}, as it should be in all
properly conducted scientific research to fulfill the basic requirement of
reproducibility.

Another point-of-view in the thesis is the quality of the spell\-/checking for
morphologically complex languages. For English, the vast majority of the
research on the topic has already shown impressive numbers on quality of
spell\-/checking and corrections with statistical approaches. It almost seems
like a scientific consensus on the solution. The common intuition,
however, raises some suspicion that the morphologically more complex languages
may not be so favorable to simple statistical approaches. The contribution of
this thesis is also in showing the limits of these approaches for
morphologically complex, and resource-poor languages, exploring possible
modifications or tricks that can be used to salvage the situation.

The remainder of this first chapter is dedicated to an informal introduction to
the thesis topic. It consists of closer definitions of the practical and
scientific concepts central to building a spell\-/checking system as a research
problem, some very down-to-earth rationale for this approach, and the
background details that motivate the research of finite\-/state methods for
spell\-/checking even when spell\-/checking itself has already almost been
researched to death. Finally, as this is an article-based thesis, I give an
overview of the articles of the thesis, and describe how the thesis is
structured.

\section{Components of a Spell-Checking and Correction System}
\label{sec:practical-components}

The spell\-/checking and correction, like the title would suggest, can already be
separated into two components. Furthermore both of the components can be
divided, classified and analysed into many separate pieces. In this section I
try to cover different existing and practical conceptualisations, and introduce
the relevant terminology in a neat and systematic manner.

The primary division of labour that is present throughout this thesis including
in the title, is the division into spell\-/checking and correction. The former is
the task of \emph{detecting} the errors in the texts, whereas the latter is the
task of \emph{suggesting} the most likely correct word forms in place of the
error. This separation is rather central to my thesis, so Imake a point to
always read the word \emph{spell\-/checking} to mean only the task of detecting
whether or not the word is correctly written, and \emph{correction}, to mean
the process where the system suggests possible corrections for the mistake.  It
is of great importance to treat these two tasks as separate, as the
implementations made for each of them can be quite freely mixed and matched.
While there are systems that use the same methods and approaches for both, this
is not by any means a necessity. A practical system giving suggestions usually
does not have access to the context of the word-form it is correcting and to
the criteria by which the word was chosen. These are relevant constraints in
some of the approaches I present in the thesis and for some of the concepts I
present in this section.

The methods for detecting errors can be divided according to the data they use
for evidence of word being \emph{misspelt}. The basic division is whether the
system looks only at the word-form itself, in \emph{isolated} spelling error
detection, or if it actually looks at the \emph{context} as well. Traditionally
the isolated error detection has been based on lookup from word-form lists
checking if the word-form is valid. The only additional notion we have in terms
of this thesis, talking about finite\-/state spell\-/checking, is that our
\emph{dictionaries} are systems capable of representing an infinite number of
word-forms, i.e., they contain not only the dictionary words, but also the
derivations and compounds necessary to reasonably spell-check a morphologically
complex language. Sometimes, but not very often, this spell\-/checking is backed
up with statistical information about words: the words that are in a
dictionary, but are very rare, might be marked as errors, if they are one
typing error away from a very common word, such as \misspelt{lave}\footnote{I
    will use this special emphasis for misspellings throughout the thesis as
    most of the users of contemporary spell-checkers should be familiar with
it.} instead of \emph{leave}~\citep{kukich1992techniques}.  The spelling errors
of this kind, where the resulting word is an existing word in the dictionary,
are called \emph{real-word} errors.  The short-coming of isolated approaches
for spell\-/checking is that they will basically only recognise those spelling
errors, which result in a word-form that is not in the dictionary, the
so-called \emph{non-word} spelling errors. The real-word spelling errors will
almost always require the use of context to get some evidence that something is
wrong. Here again, the simplest approach is to use statistics; if the word in
word we are inspecting does not usually appear in the current context of two to
three words, it may be an error. Another, more elaborate context-based method
for detecting errors is to use a full-fledged natural language processing
system that can parse morphology, syntax or other features of running text.
Such a system can usually recognise sentences or phrases that are unusual, or
even directly grammatically wrong; these systems are usually already called
\emph{grammar checkers}\footnote{A word of warning about this terminology:
    some practical systems, such as office software, will call any spelling
    checker that uses any context based approach, as opposed to an isolated
    approach, a grammar checker. This terminological confusion is not carried
over to Microsoft Word's recent incarnations, but at the time of writing Apache
OpenOffice.Org and LibreOffice do still follow the confused terminology.} and
correctors, rather than spelling checkers, and are not a central topic of this
thesis, although I do try to make it clear what kind of extensions could be
made to turn my spell-checkers into such grammar-checkers.

The error-correction task is divided most naturally by the types of errors that
are made. There have been many different classifications for these with
different names, but most will agree that there is a
category of errors that is based on unintentional mistakes, such as
mistyping at the keyboard. This is the type of spelling error that is the
main target for correction suggestion systems, and the common
research results have claimed that between 80 and 95 \% of all spelling
errors can be attributed by having \emph{one} typing error in a 
word~\citep{kukich1992techniques}. The rest of the spelling errors are more
or less based on \emph{competence}, such errors can be caused by not knowing
how to spell a word, especially common in orthographies like English, or not
knowing the correct way to inflect a word, which is common when writing in a
non-native language. Increasingly common are errors caused by limited input
methods, such as dropping accents or otherwise transcribing text to match e.g.
a virtual keyboard of a mobile phone---conveniently these errors are 
indistinguishable from a mistyping.



\section{Morphological Complexity}
\label{sec:morphological-complexity}

One theme of the thesis was to bring effective and high-quality spell\-/checking
and correction to languages that have a variety of morphological features. I
will refer to this variety as complexity, even if this term is sometimes
frowned upon with the belief that morphologically simple languages like English
would be considered less valuable---which is not the case.  To substantiate
this claim I will first try to define what I mean by morphological complexity.
The topic of this thesis is not typology nor morphological complexity, and I
will take a rather practical view on the topic.  This is a measure of
complexity that affects the building and use of correct word-form recognisers
in terms of computational and practical resources; there are only few easily
measurable factors in this playing field. The morphological complexity of the
language model is the complexity of the word-forms in terms of \emph{morphs},
i.e., the average count of morphs in the word or \emph{morph-to-word}
ratio\footnote{In the literature the term is often morpheme to word ratio. As a
morpheme is an abstraction that cannot really be counted in actual text, I
prefer to use the less misleading form of the term.}. The morphs I use here
refer to the smallest meaningful unit in the word that is used so regularly,
that it can be practically used in the language description---that is, a
productive morph. For example, the English word \emph{cat} has one morph, and
its plural, \emph{cats} has two morphs.  Another factor of morphological
complexity that is important is the variation that creates a set of related
morphs, which are encoded in an abstraction called \emph{morpheme}, for
example, the words \emph{runs}, and \emph{catches}, have suffix morphs
\emph{-s}, and \emph{-es}, which could be grouped together. This variation can
be realised in terms of the number of \emph{morphemes} per language or
\emph{morphs-per-morpheme}.

The common terminology for complexity of languages talks about \emph{isolating}
and \emph{synthetic} languages. In terms of our morphological complexity
measures isolating are languages that have an average morph-to-word ratio
around 1--2. The languages with high morph-to-word ratio are often called
\emph{poly-synthetic}. Another scale of terms is \emph{analytic},
\emph{agglutinative}, \emph{fusional}, and again, \emph{poly-synthetic}. Which
is to refer to the kind of variation the morphemes tend to have. In analytic
cases the morph-to-word ratio is low and therefore morphs do not have
variations either, the agglutinative languages have morphs that have little
variation to the extent that typical language models are concatenated from
separate morphs, and fusional languages have very high rate of variation to the
extent that variation itself is considered a new morph. 

These classifications lend themselves nicely to a view of
spell\-/checking software, where the languages that fall into synthetic and
agglutinative contain most of the languages that have their special
implementations in the world of open source spell-checkers; Finnish and Voikko,
Hungarian and Hunspell, Turkish and Zemberek, and so forth.  This tendency
suggests that people trying to build spell-checkers with limitations of
almost only word-list or statistics-based systems will not be able to reach
satisfying results, and other approaches are in fact necessary.

One, perhaps the most important form of morphological complexity relevant to
the topic of this thesis, and one that is often ignored, is the rate of
productive derivation and compounding that forms recurring structures capable
of generating infinitely long words. For a language where these processes are
common, spell\-/checking needs---at least to some extent---to support an infinite
dictionary to be useful. Surprisingly many of both existing systems, and those
described in recent research simply do not acknowledge the need. Some are
glossing over the fact as a possible extension to the system, some ignoring it
altogether but usually without any example of an implementation and evaluating
strictly based on English or a few other popular indo-european languages with
similar features.  The fact that the lexicon of many languages is infinite and
that this has practical effects has a good theoretical treatment in
e.g.~\citet{kornai2002many}.

To illustrate the practical issues with morphologically complex languages we
use the de facto open source spell\-/checking system, Hunspell. Of the
aforementioned morphological features, Hunspell supports up to 2 affixes per
word, or 3 morphemes. For languages going beyond that, including Hungarian, the
creator of a dictionary will at least need to combine multiple morphemes into
one to get a working system. The morpheme count in Hunspell is realised as
affix sets, of which Hunspell supports up to 65,000 (the exact number supported
may vary a bit in versions, but it seems to be less than 65,535). On a more
positive note, the compounding part of infinite word-lists is supported by
Hunspell.

As a concrete example of limitations with e.g. Hunspell, for Finnish there has
been at least one failed attempt to build a spelling-checker using Hunspell
\citep{pitkanen2006hunspell}, whereas we know from the early days of the
finite\-/state methods in computational linguistics that Finnish is
implementable, in fact it has often been suggested that the initial
presentation of automatic morphological analysis of Finnish in
\citet{koskenniemi1983twolevel} is one of the main factors in the popularity of
finite\-/state methods among many of the related languages. Many of the languages
with similar morphological features however have been implemented also in the
Hunspell formalism, indeed as even the name suggests, the formalism originated
as set of extensions to older *spell formalisms to support Hungarian language.
For North Sámi, there is an implementation for Hunspell. The languages that
fall into polysynthetic side of the morphological complexity are usually noted
as very hard to properly implement with the current Hunspell formalism and its
limitations, though often the results are found tolerable. E.g. for Gĩkũyũ
\citet{chege2010developing} used 19,000 words with extensive affix rules as a
spell-checker, for which they say they attained a recall of 84~\%. Same does
not seem to apply for Greenlandic, where it was found that a collection of
350,000 word-forms will only cover 25~\% of the word-forms in newspaper
texts\footnote{\url{http://oqaaserpassualeriffik.org/a-bit-of-history/}}.

The sizes of word-lists lead to the final, perhaps most practical metric of
measurement in the morphological complexity of languages in the context of
computational applications. That is, the size of the dictionary when it is in
the memory of the computer, whatever may be the method of encoding it or
compressing it; this is easily measurable and a very important factor for any
practical application.  For example for a word-list approach, even an
uncompressed the word-list of a million words with average word length of $N$
is just $N$ megabytes, storing the list in finite\-/state automaton, suffix trie
or hash, considerably less. With derivation and compounding the memory
requirements tend to go up rapidly, so there is a practical correlation measure
between the complexity of the language defined in theoretical terms earlier; in
a worst case scenario this is obviously a limiting factor, e.g. some versions
of my Greenlandic experiment took up almost 3 gigabytes of memory, which is
clearly unacceptable for a spelling checker in an average 2013 end-user
computer system.

\section{Finite-State Technology in Natural Language Processing}
\label{sec:finite-state-technology-in-nlp}

After considering the limitations and problems of the word-lists and other
simple approaches, we come to the second part of the thesis topic: finite\-/state
methods.  It is commonly believed that the regular languages that the
finite\-/state automata encode are of the same expressive power as the natural
language morphologies, which means it is also the case that all language
dictionaries can be encoded using finite\-/state automata. I have taken this
assumption at its face value when developing the thesis, and have used
finite\-/state solutions for language modelling as an axiomatically known to be
true building block, developing the idea more towards the practical application
of the spell\-/checking. In real-world applications we commonly hit the
limitations of the system in any case, and documenting these is part of good
research on the topic.

The latest developments in the finite\-/state approach to natural language
processing has been the concept of weights in finite\-/state automata.  Simply
said, I study some approaches of bringing the expressive power of statistical
language models to the traditional rule-based models. One significant part of
the contributions in this thesis studies the notions of applying statistical
approaches in conjunction with morphologically complex languages using
finite\-/state methods.

The concept of modeling typing errors is not so widely accepted as a part of
the finite\-/state technology. While for example in speech applications,
trying to map a representation of the spoken audio signal into written language
is quite common, it is relatively new in the mapping of typing errors to
correctly typed text. The contribution to finite\-/state methods in this thesis
is further research on the application of finite\-/state models of typing errors
as a practical component in a full spelling correction system.

\todo{Needs a re-write}

\section{Overview of Thesis Articles}
\label{sec:articles}

This dissertation is a collection of articles I have written during the
development of different parts of our finite\-/state spell\-/checking system. It
covers a wide range of topics, all tied together by the goal of getting things
to work nicely; catching up with the quality and speed of the spell-checkers
for languages like English. My articles, in chronological order are presented
in an unnumbered Chapter \emph{Original Papers} on page~\pageref{chap:papers}.

The development shown in the articles is easy to follow in the
\textbf{chronological order}, it almost tells its own story. In the beginning
we have a Finnish morphological analyser \citep{pirinen2008suomen}, which
cannot yet be turned into a statistical dictionary as it contains too rich
morphological productivity. In articles
\citepalias{pirinen2009weighted,pirinen2009weighting} we explore some advanced
weighting options of the Finnish morphology that could give better treatment of
compounds and derived word-forms in morphologically complex languages. In
\citepalias{pirinen2010finitestate} we try to tackle the problem of scarcity of
corpus resources especially in conjunction with morphologically complex
languages, using a crowd-sourcing option provided by Wikipedia and measuring
how smaller text corpora will still improve the spelling correction results. In
\citepalias{pirinen2010building,pirinen2010creating} I have studied the
compilation and reuse of existing non-finite\-/state language descriptions as
finite\-/state automata-based language models of a finite-state spell\-/checking
system.  Specifically in \citepalias{pirinen2010creating} I attempt to
reimplement much of the Hunspell spell\-/checking system, including their error
correction methods, in finite\-/state form. In
\citepalias{pirinen2011modularisation} I research the topic of how to maintain
finite\-/state language models, and try to show that finite-state language models
are feasible for long-term maintenance for a vast array of applications using
just one language description instead of one description per application.  I
extend this theme in \citepalias{pirinen2012compiling} by showing how to
convert language models outside the morphological analysers and spell-checkers
into a suitable finite\-/state automaton. In \citepalias{pirinen2012improving} I
tackle the problematic issue of context-aware spell-checkers for
morphologically complex and resource-poor languages, showing some practical
limitations there are and what can be done to get some benefits from such an
implementation. In \citepalias{pirinen2012effects,pirinen2013quality} we have
already reached the point of full-fledged, workable systems for
spell\-/checking, and test the full power of those systems, first for speed and
efficiency in \citepalias{pirinen2012effects}, and then in a larger scale
survey~\citepalias{pirinen2013quality}, the quality of spelling suggestions, as
well as extensions to some of the speed measurements.  The chronological order
is also visible in the schematic diagram of Figure~\ref{fig:schematic-diagram}:
the finite\-/state language models (in the leftmost package) were conceptualised
after the statistical weighting schemes of morphologically complex language (in
the middle package) and the resulting combinations were evaluated on a larger
scale (in the rightmost package).

The \textbf{division of labour} within those articles, also quite naturally
follows from the chronological development.
In~\citepalias{pirinen2009weighted,pirinen2009weighting}, as I was the author
of the Finnish finite\-/state implementation of the morphological analyser we
were extending with weight structures, I did much of the legwork in
implementing the practical software for weighting the analyser. The result of
this work is the weight support in such parts of our finite\-/state tool chain as
\texttt{hfst-lexc} and \texttt{hfst\-/strings2fst}---that is, in practice the
compilers for morphemes and other string sets. In these early articles, the
evaluation scheme and major parts of the written article are the work of
Krister Lindén.  In~\citepalias{pirinen2010finitestate} I already take into
account that previous experiments were performed using commercial corpora
unavailable for scientific, free and open source work, which lead me to devise
a statistic training as well as an evaluation scheme based on the freely
available Wikipedia data. The evaluation and statistical training setup that
was built in this article has then evolved throughout the thesis, in the form
of collected \texttt{make} recipes, \texttt{awk} scripts, \texttt{python}
scripts and other pieces of \texttt{bash} command-line programming. The
engineering work on Hunspell language models
in~\citepalias{pirinen2010building,pirinen2010creating} originated from the
compilation formulas for morphologies, documented e.g.\/
in~\cite{linden2009hfst}, and were constructed by Krister Lindén and myself.
The same technique was applied to building of the Apertium system's language
models in~\citepalias{pirinen2012compiling} with technical help from one of the
Apertium system's main contributors, Francis Tyers.  The context-based weighted
finite\-/state methods of \citepalias{pirinen2012improving} originated
from~\citet{silfverberg2010partofspeech}, with whom I also co-operated in the
porting of the system to finite\-/state spelling checker use. The evaluation
scheme was slightly modified from an earlier one used in
\citepalias{pirinen2010building}. The writing of the article
\citepalias{pirinen2012improving} was for the main part  my own work, whereas
the mathematical formulas were built in co-operation by the article's other
authors.  In~\citepalias{pirinen2012effects}, the evaluation setup and article
was written by myself, with majority of the technical implementations done by
Sam Hardwick, who is also the main author behind the finite\-/state
spell\-/checking component of our current software, as is further detailed
in~\citet{linden2011hfst}.  In the
articles~\citepalias{pirinen2011modularisation,pirinen2013quality} the majority
of the test setup, article structure, engineering and other tasks were
performed by myself, which is reflected in the fact that they are published
under my name only. It is, however, true that all of the work within this
thesis was made as member in the HFST research group, and there is no doubt
that there is an abundance of minor ideas regarding engineering of the
software, evaluation setup and article writeup that are the creation of
collective brainstorming as well.

Another way to conceptualise the thesis is under the following \textbf{research
questions}: How to implement reasonable statistical language models for
finite\-/state spell\-/checking, how to implement a finite-state equivalent of
state-of-the-art software spell\-/checking, how to develop and maintain a
language model for finite\-/state spell\-/checking, what is the quality of
finite\-/state spell\-/checking compared with a software-based approach, what is the
speed of the finite\-/state spell\-/checking compared with other software
based-approach, what are the limitations of the finite\-/state-based approaches
compared with software-based spell\-/checking---and doing all of this with
morphologically complex languages that are lesser-resourced ones.

The rest of the thesis is \textbf{organised} by topic \textbf{into chapters} as
follows: In \ref{chap:background} I will attempt to summarise the whole prior
research of spell\-/checking and correction, as well as describe the
development of the actual in-use spell-checkers to the best of my knowledge. I
will then describe the previous research specifically on finite\-/state
approaches to spell\-/checking, and describe the theory of finite\-/state
spell\-/checking in terms of our specific implementation. The
Chapters~\ref{chap:language-models}---\ref{chap:efficiency} contain the
research papers of the thesis sorted under four headings that match original
goals of this thesis.  In Chapter~\ref{chap:language-models} I go through the
existing spell\-/checking and other language models and their use as part of
the spell\-/checking system, and introduce the finite\-/state point of view to
the language models that initially were software-based.
In~\ref{chap:statistical-models} I describe some weighted finite\-/state methods
to bring statistics to the language and error models.
In~\ref{chap:error-models} I study the finite\-/state formulations of error
modeling and contrast my implementations with the others that have been used.
Finally, in~\ref{chap:efficiency}, I fully evaluate these systems on both speed
and quality to verify their usability in practical and real-world applications.
Most of the articles I have written fall neatly under one of these headings,
but I do revisit a few of them in more than one of the chapters. In the final
Chapter~\ref{chap:conclusion} I summarise the thesis and lay out the possible
future work in the field.

\begin{figure}
    \includegraphics[width=\textwidth]{diss-structure-uml-ish}
    \caption{Articles and build order
    \label{fig:schematic-diagram}}
\end{figure}

\chapter{Background}
\label{chap:background}

Spell-checking and correction by computer is a topic that is already over half
a century old, and a one that has been researched periodically throughout that
time.  In this chapter I attempt to walk through the long history of
spell\-/checking with an eye on both the scientific study of spell\-/checking,
as well as the practical end-user systems. While I attempt to cover as
much as possible of the topic, there are bound to be many omissions and for
a fuller picture I recommend reading some of the previous surveys on
spell\-/checking and correction, such as
\citet{kukich1992spelling,mitton2009ordering}.  In~\citet{kukich1992spelling}
the history of spell\-/checking, both scientific improvements and some
real-world applications is very well summed up until the publication time of
early 1990's.  In~\citet{mitton2009ordering} there are more recent advances on
non-word spelling correction.  \citet{kukich1992spelling} also refers to
spell\-/checking as a \emph{perennial topic} in computational linguistics, I
find this characterisation quite accurate as it has been a prevalent and
recurring theme throughout the history of computational linguistics and earlier
in computer science.

\todo{Pilkut: [pitkä adv], subj ... , CC eg: section X, I go}

This backgrounding chapter is organised as follows: In
Section~\ref{sec:history} I go through the history of spell\-/checking and
correction, especially the scientific contributions that relate to my research,
and practical software systems I have used as a baseline for my target
functionality.  After dealing with the history of spell\-/checking and
correction I go through the closely related scientific fields in
Section~\ref{sec:related}, whose results are relevant to my research. Finally,
I go through the current situation in Section~\ref{sec:finite-state-theory},
and specifically the finite\-/state approaches to spell\-/checking and
correction, and describe my particular system. I will also detail much of the
notation and terminology in these chapters, and contrast them to other similar
approaches.

\section{Brief History and Prior Work}
\label{sec:history}

The history of spell\-/checking and correction by computer can usually be timed
to have begun somewhere around 1960's, with inventions like Levenshtein's and
Damerau's measures of distances between strings
\citep{levenshtein1966binary,damerau1964technique}, or Damerau-Levenshtein
\emph{edit distance}, which even today is the core of practically all spelling
correction algorithms. This measure defines distance between two strings of
characters, in terms of editing operations performed on the string to match it
to the other string. The editing operations defined were the following:

\begin{itemize}
    \item \emph{deletion} of a letter
    \item \emph{addition} of a letter
    \item \emph{changing} a letter to another, and
    \item \emph{swapping} or transposing adjacent letters
        (This is omitted from some formulations and sometimes ignored in
        spelling correction applications)
\end{itemize}

For example, the distance between \emph{cat} and \misspelt{ca} is 1 (deletion of
t), the distance between \emph{cat} and \misspelt{catt} is 1 (addition of t), and
so forth.  It is easy to see from these definitions how it is useful for
spelling correction; they are the model of basic typos or slips of the finger
on a keyboard. The paper by \citet{damerau1964technique} is often cited to be saying that
95~\% of the errors are covered by distance 1, i.e. one application of this
edit algorithm would fix 95~\% of misspellings in a text. While this is indeed
the claim made in the article, citing it in context of modern spelling-checker
may neglect the difference between input mechanisms (and data transmission, and
storage) of computers of the time, although even that is dealt with in the
article. Further studies on different forms of computerised texts have shown
that the range of single edit errors is around 70~\%--95~\% in various setups
(e.g. OCR, dictating and typing, normal running text).

Much of the research on error detection, rather than correction, concentrated
on efficiently looking up words from finite word-lists and building data
structures. During this time, also the first statistical approaches, as in
\citet{raviv1967decision}, drawing from basic foundations of mathematical
theory of information from as early as
\citet{shannon1948mathematical}\footnote{I feel I must attribute
\citet{liberman2012noisily} for this tidbit of information}, were devised. In
this research the statistical theories are applied to characters of English
legal text, recognising also names and other terms. The input mode of this
application seems to be more towards OCR than keyboard input. This basic
approach took the letter n-grams and assumed that words containing unlikely
letter combinations were not spelled correctly.

For the very first spell\-/checking software in common use---there are a few
disputes and claims to it---but it is commonly attributed to the SPELL
program~\citep{gorin1971spell}.  Some of the earlier work has been used mainly
by one research group for their own purposes
only~\citep{earnest2011first,earnest2012first,earnest2013legacies}. The first
predecessors of SPELL according to Les Earnest were systems for recognising
hand-written cursive text as auxiliary parts of larger software. Following this
the SPELL was possibly among the first self-standing software for
spell\-/checking.

%%The features of SPELL are interesting, as its legacy has carried over to all
%%the spell-checkers for Unix systems that came after it up to the contemporary
%%software.  SPELL was already performing the correction using basic one error
%%edit distance kind of model, it included support of word lists and user
%%dictionaries that it could generate from missing words itself. And it also
%%included English affix stripping. The limitation of word-length was 40
%%alphabetical characters including hyphen and apostrophe. It appears that the
%%hash structure used for correction limits the search space for corrections,
%%i.e. the corrected word must have the same hash prefix as the misspelled word,
%%which will limit the search space to the letters after the 2nd letter of the
%%word. They seem to suppose that the first two letters are always correct.
%%\citep{gorin1971spell}

Much of the following research on error detection consisted of more elaborate
data structures for fast lookup and efficient encoding of large dictionaries,
while interesting as a computer science and data structures topic, it is not
mostly relevant to this thesis, so I will just summarise that it consisted of
hashing, suffix-trees and binary search trees, partitioning of dictionary by
frequency \citep{knuth1973art} and most importantly, finite\-/state automata
\citep{aho1975efficient}. While Aho's article is about constructing more
specific finite\-/state automata for efficient keyword matching it is a first
implementation towards finite\-/state spell\-/checking.

The SPELL program's direct descendant is the current international
ispell~\citep{gorin1971spell}, where the additional i originates from the name
\texttt{ITS SPELL}. It is still commonly in use in many Unix-based systems.
According to its documentation, the feature of suffix stripping based on
classification was added by Bill Ackerman in 1978, e.g. it would only attempt
to strip the plural suffix \emph{-es} for words that were identified to have
this plural suffix.  The concept of affix flags is still used in all of the
ispell's successors as well.

In the turn of 1990's there was a lot of new research on improving the error
correction capabilities, possibly partially due to rapid growth in popularity
of home computers. Most popularly the use of statistics from large text corpora
and large dataset of real errors that could be processed to learn probabilities
of word-forms and error types was applied and tested
extensively~\citep{kernighan1990spelling,church1991probability}. These popular
and simple methods are still considered to be useful for modern spell-checkers,
which can be seen as common revisions to the techniques, such as
in~\citet{brill2000improved}.

One of the digressions was to improve error models for English competence
errors. The initial work for this is the often cited Soundex algorithm,
originally meant for cataloguing names in a manner that lets you find similarly
pronounced names easily~\citep{russell1918soundex}. It can also be used to
match common words, since it is a \emph{similarity key} that maps multiple
words into one code and back, for example \misspelt{squer} and \emph{square}
have the same code \texttt{S140} and can be matched. What the soundex ruleset
is basically about, is saving the first letter and assigning rest of
non-adjacent non-vowels a number. There have been some schemes to elaborate and
make this work for foreign names and more words, most notably Metaphones
by~\citet{philips1990hanging,philips2000double}[the third is a commercial
product without publicly available documentation].

As the computational power increased, it became increasingly practical to look
again at the problem of real-word spelling error detection.
In~\citet{mays1991context} it is suggested that for English context-based
approaches detect 76~\% of the spelling errors, and are able to correct 73~\%.
The context-aware models are a requirement for the discovery of the real-word
spelling errors, but independently of that, they can also be used to improve
the corrections of the spelling errors. 

In \citet{al2006learning} it is shown that it is possible to learn the common
real-word errors directly from the text using correctly spelled reference texts
to calculate context factors for the words, and seeking words deviating from
the factors enough in the other texts, crafting some logic formulas for
replacements of the real-words.

While computation power and storage space has improved along Moore's
prediction until recently, it is still not obvious if practical word n-gram
models are light enough to be used interactively in a desktop product that is
practically in use during the whole uptime of a modern desktop system.

In the world of context-aware models, simple word-form n-grams are not the only
form of context that has been used---especially in the context of languages
other than English it has often been noted that using sequences of
morphological analyses instead of the surface word-forms is a more important
factor in detecting the errors and improving the results~\citep[for
Spanish]{otero2007contextual}. I have performed an experiment with this
approach in~\citepalias{pirinen2012improving} for Finnish.

The problems of implementing Hungarian with ispell, aspell and the like lead to
Hunspell, with multiple affix stripping and compounding added. Similarly for
Turkish, various computational methods have been used. Among
those~\citet{oflazer1996errortolerant} demonstrates one of the first
finite\-/state spell\-/checking with full-fledged finite-state language
model---the earlier by \citep{aho1975efficient} being more of a keyword search.
This method used a specialised search algorithm for error tolerance when
traversing the finite\-/state network.  In~\citet{agata2002typographical} the
finite\-/state methods were extended to cover the error-model as well. Their work
showed a practical implementation of finite\-/state edit distance, and finally in
\citepalias{pirinen2010finitestate} I have shown that an approach by an
extension to weighted finite\-/state automata for both language and error models
for spell\-/checking and correction is plausible for the morphologically
complexer languages.

% The methods proposed since then use a Bayesian approach (Golding
%(1995)) that may be combined with part-of-speech trigrams (Golding and Sch-
%abes (1996)), transformation-based learning (Mangu and Brill (1997)), latent
%semantic analysis (Jones and Martin (1997)), differential grammars (Powers
%(1997)), lexical chains (St-Onge (1995), Hirst and St-Onge (1995), Budanitsky
%(1999), Budanitsky and Hirst (2001)), and Winnow-based techniques (Gold-
%ing and Roth (1996, 1999), Roth (1998)). The two leading prior methods are
%the statistics-based BaySpell (Golding (1995)) and the Winnow-based WinSpell
%(Golding and Roth (1999))
% [[quoted from al2006learning]]

In the Table~\ref{table:history-apps} I have first summarised the practical
end-user applications of spell\-/checking, specifically of the freely available
*spell family of products. Then I summarise some of the main academic research
on specific practical and statistical advances presented
by~\citet{al2006learning} that I have used during my research. Then in the
final set of rows I have the finite\-/state spell\-/checking systems that I am
aware of.

\begin{table}
    \centering
    \begin{tiny}
    \begin{tabular}{|l|r|l|l|l|}
        \hline
        \bf Name & Year & Error Models & Language Models & Note \\
(Authors) & & & & \\
        \hline
        \multicolumn{5}{|c|}{\bf SPELL --- Unix --- FLOSS branch (*spell) }\\
        \hline
             SPELL, & 1971 & Edit-Distance 1* & Word-list, & Edit distance \\
\citep{gorin1971spell} &  & & English affixes & limited to \\
                                  &  & &              & 2 first letters \\
        ITS SPELL, & 1978 &  & Affix rules & full ED \\
     (Bill Ackerman) & & & & \\
        international ispell & 1988 & & & Non-English \\
              (Geoff Kuenning) & & & & \\
        \hline
        kspell, & 1998 & Metaphone 1 & Affix rules & \\
        GNU aspell & 2002 & Rule-weighted ED & Compounding & ED is avgd \\
    (Kevin Atkins) & & & (dropped) & w/ soundslike \\
        \hline
        myspell & 200x & weighted ED & 2 affixes & OpenOffice 1 \\
        \hline
        Hunspell & 200x & weighted ED & 2 affixes & Configurable \\
                 &      & Confusables & Compounds & edits \\
        \hline
        \multicolumn{5}{|c|}{\bf Academic projects etc.} \\
        \hline
        correct & 1991 & Prob. ED 1 & Prob. word-list & Reranker \\
        \citep{church1991probability} & & & & \\
        \hline
        bayspell  & 1995 & & & Bayesian \\
        \citep{golding1995bayesian} \\
        \hline
        WinSpell & 1999 & & & Winnow \\
        \citep{golding1999winnow} & \\
        \hline
        \multicolumn{5}{|c|}{\bf Finite-State} \\
        \hline
      \citep{oflazer1996errortolerant} & 1996 & ED & FSA & Fuzzy search\\
\citep{agata2002typographical} & 2002 & ED & FSA & Automata algebra \\
      \citep{schulz2002fast} & 2002 & ED & FSA & Levenshtein automata \\
        \citep{mohri2003edit} & 2003 & ED & WFSA & Automata algebra \\
    \citep{otero2007contextual} & 2007 & FSA & FSA & Contextual \\
      \citep{hulden2009fast} & 2009 & FSA Eqv & FSA & A* search, context \\
\citepalias{pirinen2010finitestate} & 2010 & WFSA & WFSA & Automata algebra \\
        \hline
    \end{tabular}
    \caption{History of spell-checker applications \label{table:history-apps}}
\end{tiny}
\end{table}

\section{Related Subfields and Research Results}
\label{sec:related}

There are a number of other research questions in the field of computational
linguistics that use the same methodology and face the same problems. A large
set of research problems within computational linguistics can be formulated in
some frames that are directly relevant to the issues of spell\-/checking and
correction, as most of them require a basic component from a language model and
very often another one from an error model. The language model of
spell\-/checking predicts how correct a word-form is, and in a linguistic
analyser it predicts the analysis and its likelihood. The error model of a
spelling corrector predicts what is meant based on what is written assuming an
error in the process of typing somewhere. Part of the error sources, such as
cognitive errors, overlap in speech recognition and information extraction
alike. Part of the error models for other applications like diacritic
restoration for information retrieval and noise cancellation for speech
recognition may solve different problems but the used approaches are still
applicable. Realising this all, at one point of the research made me
persistently require a modular design from our spelling correction
systems---including a strict separation of the spelling detection task, the
correction task---to be able to mix and match the approaches found in various
sources to the spelling correction. The rest of this subsection I use to
briefly introduce the specific approaches and algorithms from outside the
spelling detection and correction domain that I have re-used or tried to
re-use in my experiments on spell\-/checking and correction.

The relevant research on the finite\-/state approaches to predictive text entry
is referred to e.g. in~\citep{silfverberg2010partofspeech,forcada2001corpus},
and the results of
these applications have been applied verbatim to our language models where
applicable.

A highly relevant sub-field of computer science is string algorithms,
specifically \emph{approximate string matching}. In practice, even this thesis
would be a suitable contribution to the field of approximate string matching,
as it strongly builds on the algorithms developed within this domain. The
approximate string matching using fuzzy search algorithms in finite\-/state
automata is one of the implementations I often contrast my work with, as I have
routinely replaced the fuzzy search algorithm by a composition of
well-specified fuzziness and n-best search.

In many practical systems a spelling corrector or its weak equivalent is used
for pre-processing. This phase of processing is often called e.g.
\emph{normalisation}. Many approaches of the normalisation are directly
relevant to spelling correction, and parts of the spell\-/checking system I
have developed have been getting ideas from the development of systems such as
SMS normalisation to standard written language~\citep{kobus2008normalizing}.

\section{Theory of Finite-State Models}
\label{sec:finite-state-theory}

Finite-state models for spell\-/checking and especially correction are relatively
new---beginning from \citep{oflazer1996errortolerant} as far as I know---, and
definitions and interpretations have not been standardised yet, so in this
chapter I will try to go through the various definitions and explain my
solution and how I ended up with it. To begin with, I will use a few paragraphs
to recap the formal definitions of finite\-/state systems and my selected
notations. These should be familiar to anyone who has dabbled in weighted
finite\-/state theory e.g. from \citet{aho2007compilers,mohri1997finitestate}.

The automata are conventionally marked in computer sciences and mathematics as
systems, such as n-tuple $(Q, \Sigma, \delta, Q_i, Q_f, W)$, where $Q$ is the
set of the states in the automaton, $\Sigma$ is the alphabet in transitions,
$\delta$ is the transition mapping, and $Q_i, Q_f$ the subsets of states for
initial states, and final states and $W$ is the structure of weights. The
$\Sigma$ set in the automata of spell\-/checking system is almost always just
some---usually language-specific for optimisation reasons---subset of the
Unicode set of symbols for writing natural languages, with the addition of the
following special symbols, which have specific meaning in automata theory: the
empty symbol epsilon $\epsilon$ that matches zero-length strings on application
of the automata, the wild card symbol $?$ that matches any one symbol of
$\Sigma$ during any application of the automaton.  When talking of two-tape
automata\footnote{I will systematically \emph{avoid} the term transducer in
    this thesis, a two-tape automaton is not special enough to warrant the use
    of a special term over any other specialisation of an automaton. Using the
    word transrducer will only confuse the reader to think that there is
something more to it.}, it merely means that the alphabet is of the form
$\Sigma^2$, otherwise two-tape automata are practically the same as single-tape
automata for our purposes. The weighted automata talked about throughout the
thesis are using the \emph{tropical semiring} weight structure $(\mathbb{R}_+
\cup \infty, min, +)$; this is the so-called penalty weight structure, which
practically means that on application and weight combination the smallest one
is used, on combination of the weights they are added together. The set of
final states is extended with a final weight function $\rho$ that specifies
an additional weight to the path with each end state. For the rest of the
finite\-/state algebra I use the standard notations which to my knowledge do not
have any variation that requires documenting in this introduction (e.g.  $\cup$
for union, $\cap$ for intersection and so forth).

A finite\-/state automaton, with a $\Sigma$ set drawn from the set of natural
language alphabets, such as letters A through Z, digits 0 through 9 and the
punctuation marks hyphen and apostrophe, can accurately encode some words of
the English language in the accepting paths of the automaton. These kind of single-tape
finite\-/state automata recognising words of a language are used both in process
of detecting spelling errors of non-word type in a running text, and matching
the misspelt word-forms to the correct word-forms. The use of such automata as
language models is documented very extensively in natural language processing;
in the context of morphologically complex languages more relevant to the topics of
this thesis, the relevant reference reading would be the Finite-State Morphology~\citep{beesley2003finite,beesley2004morphological}.

I use the finite\-/state automata also in error-correction. In this case the
finite\-/state automata are two-level automata that encode the relations from
misspellings to corrections in their accepting paths. There are few influential
works in the field of finite\-/state methods for the error modeling. The initial
work may have been laid out by~\citet{oflazer1996errortolerant}, who uses an
algorithmic approach on the language model traversal as a limited form of error
modeling; this has been extended and improved e.g.  by~\citet{hulden2009fast}. 

An approach to weighted finite\-/state systems was described
by~\citet{mohri2003edit}. His definitions are slightly more technical and
different from the ones used in my work.\todo{explain mohri's definitions and
differences.}

In~\citet{agata2002typographical} the finite\-/state error model was first
described as an automaton. In that paper the research concentrates on the
concept of expanding the language model automata by a Levenshtein rule
automaton, creating a language model automaton that can recognise word-forms
containing a given number of Levenshtein type errors, i.e. all word-forms at a
given Levenshtein-Damerau distance. My definitions diverge here by considering
the error model as a separate, arbitrary two-tape automaton; this allows
operating on either the language model or the misspelt string with the error
producing or removing functionality of the model, and provides an opportunity
to test which variation is the most effective. 

Now we can also make the generalisation to consider the strings of the language
that we are correcting as a single-path single-tape automaton consisting of
just one word, so we can formally define an unweighted finite\-/state spelling
correction system as the composition $(w \circ M_e \circ M_c)^2$, where $w$ is
the word to correct, $M_e$ the automaton encoding the error model, and $M_c$
the automaton encoding the language model, and $^2$ is the projection selecting
the results from the second tape of the final composition, i.e., the one on the
language model side.

The weights in weighted finite\-/state automata are used to encode the preference
in spelling correction, and sometimes also acceptability in the
spell\-/checking function. A path that has a larger collected weight gets
demoted in the suggestion list and the ones with smaller weights are promoted.
The weight can be amended by the error model, in which the weight expresses the
preference on errors corrected when mapping the incorrect string to correct.
One of my contributions throughout the thesis is the theory and methodology for
creating, acquiring and combining these weights in a way that is optimal for
speed, and that is usable for morphologically complex languages with limited
resources.

The baseline for acquisition of the weights for language and error models is
simply calculating and encoding probabilities---this is what most of the
comparable products do in spell\-/checking and correction, and what has been
researched in statistical language models. The key formula for conceiving any
statistical process in the tropical semiring giving probabilistic distribution
$P$ as a part of a finite\-/state automaton is $-\log P$, i.e. the smaller the
probability the bigger the weight. The probability here is not
straight-forward, but relatively simple. For most things we calculate $P(x) =
\frac{f(x)}{\sum_{z \in \mathcal{D}} f(z)}$, where x is the event whose
probability to count, $f()$ is the frequency of the event, and $\mathcal{D}$
the collection of all events, so the probability of $x$ is counted as the
proportion of the event $x$ in all events $z$ of $\mathcal{D}$. For example,
for a simple language model, $x$ could be a word-form, and $\mathcal{D}$ a
corpus of running text turned into word-forms, then we would expect that
$P('is') > P('quantitatively')$, for any reasonable corpus of the English
language. Similarly for error modeling, $x$ might be a typo $a:s$, and for an
error corpus of typos written with a qwerty layout keyboard we would expect
$P(a:s) > P(a:l)$.

An important factor that morphologically complex languages bring to the concept
of statistical language models is obvious, the amount of different plausible
word-forms is greater, the data is more sparse, and essentially language models
turn from finite word-form lists to infinite language models.  There is a good
mathematical-lexicographical description of this problem in
\citep{kornai2002many}. This means that for simple statistical training models
the amount of unseen words arises, and unseen words in naive models mean a
probability of $0$, which would pose problems for simple language models, e.g.
given the above weight formula $-log(0) = \infty$ for any given non-zero-size
corpus. A practical finite\-/state implementation will regard infinite weight as
a non-accepting path even if it were otherwise a valid path in the automaton.
The traditional approach to deal with this is well known; assuming a
probability distribution we can estimate the likelihoods of the tokens that
were not seen, discount a bit of the probability mass of the seen tokens, or
otherwise increase the probability mass and distribute it among the parts of
the language model that would have been unseen. With language models generating
infinitely many word-forms the models may often not fit their probability
distributions. In practical applications this does not necessarily matter as
the preference relation still works. A bit of this background on not following
strict statistical distributions in NLP context has been given by Google in
their statistical machine translation work e.g.  in~\citet{brants2007large}.

The basic forms of estimating and distributing the probability mass to account
for unseen events has been researched extensively. The basic logic that I have
used in many of my research papers is the simplest known additive discounting:
here the estimated probability for an event $x$ is seen as $P(\hat{x}) =
\frac{f(x) + \alpha}{\sum_{z \in \mathcal{D}}(f(z) + \alpha)}$ that is, each
frequency is incremented by $\alpha$ and this mass of increments is added to
divisor to keep distribution in probabilistic bounds---this has the effect that
all unseen tokens are considered to have be seen $\alpha$ times and all others
$\alpha$ more times than they have been seen in the training data. For values
of $\alpha$ in my practical applications I have used the range 0.5--1.



\chapter{Language Models}
\label{chap:language-models}

In this chapter I will go through the various kinds of language models that are
used for the task of spell checking and correction.  Depending on the system,
these two tasks can either share a common language model, or use separate and
different language models, or even different approaches to apply the language
models. This separation is an important one and I will try to make it clear
whenever I refer to language models in various systems and aspects of the
finite\-/state spell\-/checking.  The primary purpose of a language model in
both of these tasks is similar: to tell whether a word-form is suitable, and,
ideally, how suitable it is.

The purpose of this chapter is to present the development through the
traditional simple word-list spell\-/checking models to complex morphological
analysers with compounding and derivation, and their finite\-/state formulations.
This follows from my initial goal to not only present new language models and
spell-checkers in my thesis, but to use finite\-/state technology to repeat the
existing results of non-finite\-/state spell-checkers as a baseline. In the later
chapters I will show that the finite\-/state formulations are also as good as, or
better than their algorithmically processed counterparts, and the statistical
methods we devise to support morphologically complex languages can be applied
to these finite\-/state automata.

The rest of the chapter is organised as follows: First in
Section~\ref{sec:generic} I introduce some generic formulas for finite\-/state
morphologies we have recognised when developing compilers for quite a few
dictionary formats. Based on this, I show the finite\-/state compilation of
Hunspell dictionaries, the de facto standard of open source spell\-/checking in
Section~\ref{sec:Hunspell}.  Then I show the conversion of the rule-based
machine translation dictionaries as spell-checkers in
Section~\ref{sec:apertium}. In section~\ref{sec:other-lms} I describe other
language models that can be used in finite\-/state spell\-/checking. Finally, in
Section~\ref{sec:maintenance}, I introduce a brief general treatment on the
management of linguistic data to have it apply on a multitude of projects in an
article where I try to tie together the different approaches to compiling and
using the finite\-/state dictionaries and computational linguistic models at
large.

\section{Generic Finite-State Formula for Morphology}
\label{sec:generic}

Before delving further into the intricacies of compiling existing and new
formalisms into finite\-/state automata, I would like to draw users attention to
one underlying formula of computational morphology that is central to all these
approaches. The conceptualisation that all systems are regular combinations of
sets of morphemes, combined with rules of morphotactics and possibly
morphophonology.  The basic starting point is an arbitrary combinations of
morphs as an automaton $\mathcal{M}_{morphs} := \bigcup ({morph})^\star$. The
set of morphotactic rules over the classifications of morphs (e.g. hidden in
the automata in the boundaries of morphs as special symbol arcs) is always
construed over $\mathcal{R}_{morph} \in \Gamma^\star << \Sigma^\star$ where
$\Gamma$ is the boundaries of morphs such that $\Gamma \cap \Sigma =
\emptyset$. The morphophonology likewise is an automaton $\mathcal{R}_{phon}
\in \Sigma \times \Sigma$ modeling variation in the characters, possibly bound
to morpheme boundaries. If you follow this train of thought that all formulas
generalise to this formula, the chapter will be more straightforward to follow.

When dealing with finite\-/state language models for other purposes, e.g.
analysis of language, the compilation formula is exactly the same as shown for
the Lexc language in e.g.~\citet{linden2009hfst}, keeping in mind that the
second tape means cross-products in the formulas above. In the resulting
language, the language model for finite\-/state spelling correction only needs
the surface tape of the automaton, so it is possible to make a projection, or
use composition instead of intersection in the application phase of the
finite\-/state spelling correction.

\section{Compiling Hunspell Language Models}
\label{sec:Hunspell}

\textbf{Main Article}: \emph{Building and Using Existing Hunspell Dictionaries
and \TeX\ Hyphenators as Finite-State Automata} by myself and Krister Lindén.
In this article we present finite\-/state formulations of existing spell\-/checking
language and error models.

The \textbf{motivation} for this piece of engineering is far-reaching both on
the technical and theoretical level. We set out to prove that the existing
methods for spell\-/checking are truly a subset of the finite\-/state methods.
In practice we also tried to show that typically, if not always, the
finite\-/state version should be faster than the software-based version, e.g. for
affix stripping for sure, but also for error modelling. The practical
motivation for the work is that there is no chance for any new spell\-/checking
system to survive, unless it is capable of profiting from the existing Hunspell
dictionaries.

In \textbf{related works}, there has, to my knowledge, only been, the following
attempt to use Hunspell data as automata: a master's thesis project partially
guided by Hunspell maintainers~\citep{greenfield2010open}.
%, and a generalised
%restriction compilation formula by Anssi Yli-Jyrä for much more efficient form
%of compiling the finite\-/state automata from the Hunspell roots and affixes. 
The
original Hunspell has been dealt with in an academic context 
by~\citet{tron2005hunmorph}.

There is a number of meaningful \textbf{results} in our article. It
is a central part of the thesis in that it provides the empirical proof that
finite\-/state spell\-/checking is a proper superset of Hunspell's algorithmic 
approach in terms of expressive power, and the empirical results also suggest
improvement in speed across the board. The results on speed are detailed in my
later articles and also in the Chapter~\ref{chap:efficiency} of this thesis,
so here I will first concentrate on enumerating the parts of the first
result---the expressiveness and faithfulness of the finite\-/state formulation of
the Hunspell systems.

Hunspell's language model writing formalism clearly shows that it is based on
the main branch of the word-list-based ispell spell-checkers. Words are
assigned flags that combine them with affixes, here the concept of affixes is
extended with context restrictions and deletions, but nevertheless the same
affix logic as its predecessors. The additional features brought to Hunspell to
set it apart from previous versions were the possibility to have more than one
affix per root---two for most versions, and then a number of various extra
features based on the same flags as used for affixation, such as compounding,
offensive suggestion pruning, and limited circumfixation.  A majority of these
features come from the basic formula we use for a bag of morphs and filters
style finite\-/state morphology as it is defined in~\citet{linden2009hfst}. The
practical side of the main result shows that all of the language models are
around the same size as finite\-/state dictionaries and morphologies made with
other methods.

When talking about Hunspell it is not possible to avoid the motivation behind
the development of a new system for spell\-/checking---as my thesis is also
about a new system for spell\-/checking. Hunspell was made on the basis that
the existing spell\-/checking dictionary formalisms are insufficient to write a
Hungarian dictionary efficiently. Similar systems were written for many of the
morphologically complexer languages. 

%For the language coverage in language
%models however the compilation of e.g. Hunspell dictionaries here could be
%read as a good evidence on more general applicability conversion.

As the implementation framework for context restrictions and deletions for the
Hunspell affix morphotactics we used twol rules~\citep{karttunen1992two}. With
the context restrictions and its combinatorics twolc rules are well within
reason, as restriction is the central rule type in design of twol, whereas the
deletions are not very easy to implement in that formalism. As an afterthought,
a better and more efficient representation might have been to combine
morphotactic filters with replace style rules~\citep{karttunen1995replace},
although neither the compilation time nor the clarity of the algorithms are a
key issue for one-shot format conversions like this.

One of the main stumbling blocks for some of the finite\-/state systems with full
language models is that---while the running of finite\-/state automata is known
to be fast---the building of finite\-/state automata can be lengthy. This did
not appear to be an issue for the practical systems we tried.


\section{Using Rule-Based Machine Translation Dictionaries}
\label{sec:apertium}

\textbf{Main Article:} \emph{Compiling Apertium morphological dictionaries with
HFST and using them in HFST applications} by myself and Francis M. Tyers. In
this engineering oriented article we experiment with the existing language
models used for machine translation as a part of the finite\-/state
spell\-/checking system.

The main \textbf{motivation} for doing this experiment was to see how well we
can re-use the vast amounts of existing dictionaries from other projects as
finite\-/state automata in completely unrelated language technology systems.
This is especially interesting from my computer science background, since
the re-usability of code and data is something that I feel is lacking in
computational linguistics even now in 2010's.

As this experiment was built on an existing, partially finite\-/state-based
system, the \textbf{related works} already consist of a specific algorithm for
building the automata from XML dictionaries~\citep{rojas2005construccion} and
its features for fast and accurate tokenising and analysing. To that end, our
experiment does not provide any significant improvements, the final automata
are nearly the same and the tokenisation algorithm used to evaluate the result
closely imitates the one described in the paper.

The compilation formula built in the article was merely a simplification of
the ones used in Hunspell and lexc, making it relatively straightforward and
short as a technical contribution. The simple generalisation here is notable
however, as the computational side is based on a solid standardised XML
format for dictionary representation and interchange, it makes it optimal
for future experiments.

The main \textbf{results} shown in the paper are the improvement on text
processing time, which is in line with previous
results~\citep{silfverberg2009hfst}, and, more importantly to the topic of this
thesis, results showing reasonably fast processing time of the finite\-/state
spelling checkers built in this manner; this is not totally unexpected as the
formalism does not support compounding or recurring derivation, and thus the
final automata are bound to be acyclic. 

The one result that we tried to high-light in the paper was that we could
already provide a rudimentary spell-checker based on lexical data built solely
for the purpose of limited range machine translation, and we could do that for
a range of languages that, at the time of writing lacked spell-checkers
altogether. And the task of writing the compiler for this existing format based
on other morphology formalisms we had made and turning the compiled language
model to a baseline spell-checker was not more than a few days work. This point
in generic usefulness of finite\-/state systems is often easily overlooked with
an assumption that all new systems require months of development time.

A side result that I partially failed to communicate in this paper is, the
development towards a generalised compilation formula for finite\-/state
dictionaries. The compilation formula in this article is a variation of the
earlier formulas I have presented~\citep{linden2009hfst} and
\citepalias{pirinen2010building}---I propose that this has two implications:
for engineering side, we can use the same algorithms to compile all the
different dictionaries, which means that the automatic improvements and
optimisations that can be applied to the structure of the automata will improve
all applications. More importantly, it means that the different application
formalisms for writing dictionaries use the same linguistic models and the same
kind of (lack of) abstraction is present. This suggests that there is a
potential for generalisations such that different applications could converge
using a single, linguistically motivated dictionary format.

\section{Other Possibilities for Generic Morphological Formula}
\label{sec:other-lms}

There is an abundance of formalisms for writing dictionaries, analysers and
language technology tools available, and I have only covered the few most
prominent for the use in finite\-/state spell\-/checking. For example I have
shown a compilation formula for the Hunspell formalism. The aspell and ispell
formalisms are simplifications of this, and the formula can easily be
simplified for them by removing the multiple suffix consideration and
compounding loop in the bag-of-morphs construction: $\mathcal{M}_L :=
\mathcal{M}_{prefix} \mathcal{M}_{roots} \mathcal{M}_{suffix}$.

It should also be noted that the common baseline approaches using word-form
lists or such spell\-/checking corpora, such as~\citet{norvig2010howto}, are even
more trivial to formulate in this formula: $\mathcal{M}_L :=
\mathcal{M}_{wordforms}$.

\section{Maintenance of the Language Models}
\label{sec:maintenance}

\textbf{Main Article:} \emph{Modularisation of Finnish finite\-/state language 
description—towards wide collaboration in open source development of
morphological analyser} by myself. This short paper is mainly an opinion
piece style article, but it is included here as it ties together the
aspect of maintainable language models for spell\-/checking applications.

One of the main \textbf{motivations} for writing this article was the
realisation after some years of work on the finite\-/state language model
compilation and harvesting different computational dictionaries and
descriptions of morphologies that there is a common pattern in all the
data---as one generalised finite\-/state algebraic compilation algorithm works
for all of them. An opposite but exactly as strong motivation was that for
lesser resourced languages a single computational language model is a very
precious resource that needs to be re-used for all possible applications, as
there are no human resources to rewrite these descriptions separately for each
application. This should motivate computational linguists in general to work
towards re-usable, well-written resources, with proper abstraction levels and
the basic knowledge representation.

Another part of motivation is that the language descriptions, whether for
finite\-/state morphological analysis or Hunspell spell\-/checking are typically
written by one linguist, and left to rot as no one can read them any longer.
This is a very unuseful approach in the crowd-sourcing world of today, and it
could surely be alleviated by simple improvements to formalisms and practices.
In this branch of my research I argue for building more crowd-sourceable
systems, in the form of linguistically motivated data that is understandable
for anyone fluent in the language.

Looking into the \textbf{related works}, I soon realised that I was not
totally, though admittedly mostly, alone in my feeling that the finite\-/state
and other computational language descriptions need a bit of thought to be
maintainable and re-usable. The three notable computational linguists writing
on the same topic that I came across immediately when researching the topic
were~\citet{maxwell2008joint}, and~\citet{wintner2008strengths}. The common
features of all our concerns include indeed the lack of abstraction, and
therefore very application specific hacks in language descriptions that could
ideally be very generic. This also leads me to suggest that some related works
that are very under-used in the field of \emph{computational} linguistics are
the basic schoolbook materials on computer science and software engineering:
Knuth's pursuit on literate programming~\citep{knuth1984literate} for
well-structured and documented programs---as that is what computational
language models really are, rather than just data. And the whole
object-oriented programming movement, for encapsulation, abstraction and
management of the data for re-usability and other principled purposes.

The main \textbf{results} of this paper are seen on the continuity of the
Finnish language model and its relatively good amount of repurposing over the
years.  Beyond that, the common movement towards maintainable and re-usable
language models in computational linguistics, and morphology specifically is
still very much work in progress.

\section{Conclusions}

In this chapter, I have shown that the vast majority of the different
contemporary approaches to spell\-/checking can be formulated as finite\-/state
automata, and they can even be compiled into one using one simple generic
formula. These results, with the addition of all those existing finite\-/state
language models that no one has been able to turn into e.g. Hunspell
dictionaries should provide a basis for evidence that finite\-/state
spell\-/checking is a plausible alternative for traditional spell\-/checking
approaches at least as far as modelling of correctly spelled language. Finally,
I've described a way forward with finite-state morphologies as maintainable
form of electronic dictionaries for use of a number of applications including
spell-checking.

\chapter{Statistical Language Models}
\label{chap:statistical-models}

One of the goals of my thesis was to bring the statistical language models
closer to usability for morphologically complex languages with lesser
resources. For this purpose I tried out the most typical traditional
statistical approaches turning them into finite\-/state form using well-studied
algorithms of weighted finite\-/state automata. With the more morphologically
complex languages, the improvement gained by bringing statistical data to the
language models in the system is less encouraging than for English. I then
experimented with various ways of improving the results by more efficient use
of linguistic data and simply fine-tuning the parameters.

The problem that arises from the combination of morphologically complex
languages and the lesser-resourced languages is challenging, since the amount
and quality of the training data is smaller and the requirements for it are
greater. I go through two separate approaches to tackle this. The first
approach is to gather more linguistically motivated pieces of information from
the data as the primary training feature rather than the plain old surface
word-forms. This is not free of problems, as many good pieces of information
can only be extracted from analysed, disambiguated language resources,
which requires manual labour to produce, whereas surface word-forms
come without extra effort. So instead of requiring manually annotated data I
have studied how to salvage what can be easily used from running texts without
the need of analysis---for Finnish this meant word-forms and roots for compound
parts. The other approach I took was trying to see if even a limited
resource of lesser quality---that happens to be available under a free and open
licence---can be used combined with a well-formed, rule-based material to
greatly improve the results in error correction.

In this chapter I will describe the statistical methods I needed to introduce
to get good statistical language models for morphologically complex languages
such as Finnish, but most approaches could be applied yo others with a
few simple modifications. In Section~\ref{sec:statistical} I will introduce the
general format of all statistical finite\-/state models.  In
Section~\ref{sec:compounding} I discuss the basic findings of the compound boundary
identification problem, and in Section~\ref{sec:training-compounds} I extend
this approach towards generic statistical training of language models with
compounding features. In Section~\ref{sec:hunspell-training} I use the same
statistical models with the Hunspell dictionaries turned into finite\-/state
automata. Finally in Section~\ref{sec:lesser-training} I study the common
problem with the combination of fewer resources and more complex morphology
actually requiring more resources, and propose some possible solutions.

\section{Theory of Statistical Finite State Models}
\label{sec:statistical}

A basic statistical language model is a simple structure that can be
\emph{trained} using a corpus of texts that contains word-forms. There is ample
existing research for statistical and trained language models for various
purposes, and I strongly recommend basic works
like~\citet{manning1999foundations} as a good reference for the methods and
math background in this chapter.  For spelling correction a simple logic of
suggesting a word-form that is more common before one that is less, is usually
the best approach in terms of precision, and in the majority of the cases leads
into acorrect suggestion. This is formalised into the weighted finite\-/state
form by a simple probability formula: $w(x) = -\log P(x)$, where $w$ is a
weight in tropical semi-ring structure and $P(x)$ is the probability of
wordform $x$.  The morphologically complex languages are thought to be more
difficult to train because the number of the different word-forms is much
larger than for morphologically simple languages, which leads to decreased
discriminative capabilities for the probability calculations, as the number of
word-forms with the exactly same amount of word-forms increases. In my research
I have studied some approaches to deal with this situation. The thing that
further worsens this situation is that morphologically complex languages often
have less resources.  Much of the contributions I have provided here
concentrate on using the scarce resources in a way that will give nearly the
same advantage from the statistical training as one would expect for
morphologically poor languages with more resources.

The more complex statistical language models, such as the ones that use
morphological, syntactic or other analyses, require a large amount of manually
verified data that is not in general available for the majority of the world's
languages. In my research I present some methods to make use of the data that
is available, even if it is unverified, to improve the language models, when
possible. In general I try to show that the significance of analysed data is
not essential to typical spell\-/checking systems, but more significant for
fine-tuning of the final product and building of the grammar-checking systems
out of spell-checkers.

Another significant piece of information in statistical models is
\emph{context}. The basic statistical probabilities I described above work on
simple frequencies of word-forms in isolation, these kind of models are called
e.g. \emph{unigram} or context-agnostic language models. The common language
models for applications like part-of-speech guessing, automatic translation and
even spell\-/checking for real word errors typically use the probabilities of
e.g.  word-forms in context, taking likelihood of word-form being after its
previous words as the probability value. These are called \emph{n-gram}
language models.  The requirement for large amounts of training data is even
more steep in models like these than it is with context ignorant models, so I
will only briefly study the use of such models in conjunction with finite\-/state
spelling correction. One has to be wary though, when reading these terms
throughout this chapter: I will commonly apply methods of n-gram language
models to make unigram language models of complex languages work better,
practically treating morphological sub-parts as components of n-gram model the
way one would treat word-forms for n-gram word models of English. I will
commonly refer to the resulting models as unigram while the underlying models
in reality are a hybrid of both.

The main theme to follow for the rest of the chapter is the concept of workable
statistical models for morphologically complex languages with small amounts of
the training data available. The improvements are provided in style of
\emph{tricks} or hacks to the regular contemporary algorithms, ways to cleverly
reuse the existing scarce data as much as possible while sticking to the well
established algorithms of the field otherwise.

\section{Language Models for Languages with Compounding}
\label{sec:compounding}

\textbf{Main article}: \emph{Weighted Finite-State Morphological Analysis of
Finnish Inflection and Compounding} \citepalias{pirinen2009weighted} by Krister
Lindén and myself, introduced the statistical models of a morphologically
complex language with an infinite lexicon.

The original \textbf{motivation} for this research was to implement the
compound segmentation for Finnish given the ambiguous compounding of the
previous implementation~\citep{pirinen2008suomen}. In the article we
established that Finnish as a morphologically complex language requires special
statistical handling for its language models to be in the same ballpark as
simpler statistical models for morphologically simpler languages.

In \textbf{related works}, the compound segmentation problem has been solved
for Swedish language by systematically selecting the compounds with fewest boundaries~\citep{karlsson1992swetwol}, and for German by using statistics
of words~\citep{schiller2006german}. In the article we showed a generalisation
to both approaches, where the statistics are learned from the surface
word-forms in running text, instead of a pre-analysed and disambiguated corpus
and a manual selection routine.  Showing that this method worked built two
important contributions to the language models of morphologically complex
languages: firstly, the learning can be made from running unanalysed texts,
which is crucial, because the lack of good materials for most of the
lesser-resourced languages, and secondly, the application of basic statistical
formulas to morphologically complex languages was an important generalisation.

The \textbf{results}, as stated in the motivation are that, the experiment
framed in article was built on disambiguation of the compound boundaries,
rather than building language models for general applications. The weighting
mechanism however was applied to the language model of Finnish for all the
future research in a wider set of topics.

In the article I study the precision and recall of the compound segmentation
of Finnish compounds using the statistical language modeling scheme. The
results shown in the article are rather promising; nearly all compound
segmentations are correctly found even with the baseline approaches of giving
rule-based weights to morphological complexity. There is a slight improvement
when using the statistically formed system that gives a higher
granularity in a few cases. There is one caveat in the material used in the
paper, since Finnish lacks a gold standard material for any analysed text
corpora, we used a commercial corpus made by another combination
of automatic analysers. This means that the precision and recall values we
get are measuring how closely we imitated the referent black box system, rather
than real world compound segmentations.

\section{The Statistical Training of Compounding Language Models}
\label{sec:training-compounds}

\textbf{Main Article}: \emph{Weighting Finite-State Morphological Analyzers
using HFST tools} by Krister Lindén and myself. In this article we set out
to provide a generic formulation of statistical training of finite\-/state
language models regardless of complexity.

\textbf{Motivation} of this piece of research was to streamline and generalise
the process of creation of the statistical finite\-/state language models, as the
experiment in the previous article \citepalias{pirinen2009weighted} was built
in a very ad hoc manner. The intention of the new method for training the
language models is to get all the existing language models trainable regardless
of how they are built; this makes the process of training a language model very
similar to what it has been in all non-finite\-/state software to date.

In \textbf{related works} there is the whole field of literature of statistical
language models per se. The concept of having good language model and using
data from corpora to train it is the very basics of any form of statistical
natural language engineering. For common established software in the field, see
e.g.  SRILM~\citep{stolcke2002srilm}.  The existing work on training infinite
dictionaries is more rare, as it is not needed for English. For German this is
dealt with in \citep{schiller2006german}, which also uses finite\-/state
technology in its implementation.

As an initial \textbf{result} in the article, I replicated the earlier compound
segmentation task from~\citepalias{pirinen2009weighted}, to show that the generalised formula works.
Furthermore I experimented with the traditional POS tagging task to see how the
compound ambiguity actually affects the quality of POS tagging. The result
shows an improvement which proves that statistical word-based compound
modeling, is actually a useful part of a statistically trained morphological
analyser. This has the practical implication that it is indeed better to
use a compound-based model as baseline statistical trained analyser
of a language, rather than simple word-form-based statistical model.

In general the unigram language model with the addition of word-form-based
training for compounds shows good promise for training
morphologically complex languages from the morphologically relevant parts
instead of plain word-forms. Based on these results I moved towards an 
understanding that the statistical method of weighting morphologically complex
language models based on the word constituents of compound forms is a way 
forward for statistically training morphologically complex language models.

As conceivable \textbf{future research}, there is a generalisation to this
method I have not yet experimented with; the constituents that I used for
Finnish were word-forms, this is based on the fact that Finnish compounds are
of a relatively simple form: the initial parts are the same as surface forms of
regular singular nominatives or genitives of any plurality and in some cases
any form. This is similar to some other languages, such as German and Swedish.
There are some forms with a \emph{compositive} marker in compound forms that is
not available in isolated surface forms, and there are q few of those in
Finnish as well, e.g.\/ words ending in some `-nen' final derivation will
compound with `-s' form not seen outside compounds in addition to some archaic
compositives exist for a few words.  In languages like Greenlandic, compounding
does not heavily contribute to the complexity, but rather recurring inflection
and derivation. The generalisation that might be needed is to train the
language model based on \emph{morphs} instead of the uncompounded word forms.
I believe this could make statistics of languages of varying morphological
complexity more uniform, since the variation in amount of morphs between
languages is far more limited than for word-forms. There is a previous research
on such language models based on not the morphs, but rather letter n-graphs, or
statistically likely affixes in~\citet{creutz2005morfessor}.


\section{Weighting Hunspell as Finite-State Automata}
\label{sec:hunspell-training}

\textbf{Main Article}: \emph{Creating and Weighting Hunspell Dictionaries as
Finite-State Automata} by myself and Krister Lindén. In this article we present
weighted versions of finite\-/state Hunspell automata. This is also the main
article introducing the weighted finite\-/state spell\-/checking for first time
on a larger scale.

The main \textbf{motivation} for this experiment was to work on extending the
previously verified results for weighting finite\-/state language models to newly
created formulations of Hunspell language models as finite\-/state automata. The
functionality of this approach could be counted as further motivation for
moving from a Hunspell software approach to weighted finite\-/state automata with
the added expressive powers and efficiency.

In \textbf{related works} describing probabilistic or otherwise preferential
language models for spell\-/checking, the main source for current version of 
context-insensitive spelling correction using statistical models comes
probably from \citet{church1991probability}.

The main \textbf{results} in the article were measured from a large range of
Hunspell languages for precision, showing improvement of quality across the
board with a simple unigram training method and the Hunspell automata. A
methodological detail about the evaluation should be noted, the errors were
constructed using automatic methods, in the vein of ones researched in
\citep{bigert2003autoeval,bigert2005automatic}. This gives a clear indication
that the statistical approach for language models of spell\-/checking improves
the overall quality in its most straightforward setup.

The implication of these results for the larger picture of this research is to
show that the finite\-/state formulation of Hunspell language models is a
reliable way forward for Hunspell spell-checkers, providing the additional
benefit of statistical language modeling to Husnpell's well-engineered
rule-based models. This is important for practical development of the next
generation spell\-/checking systems, since the creation of dictionaries and
language models requires some expert knowledge, which is scarcely available for
most of the lesser-resourced languages.

\section{Lesser Resourced Languages in Statistical Spell\-/Checking}
\label{sec:lesser-training}

\textbf{Main Article}: \emph{Finite-State Spell-Checking with Weighted Language
and Error Models} by myself and Krister Lindén. In this article we first
researched the problem from the point of view of less-resourced languages. Even
though Finnish does not have significantly more freely usable resources this
is the first explicit mention of the problem in my research. Furthermore, I
used the North Saami for the first time to show the limitedness of language
resources for a real lesser resourced non-national language.

The \textbf{motivation} for this article was to write a publication that
explicitly explores the difficulties that morphologically complex and lesser
resourced languages face when trying to pursue statistical language models and
corpus-based training. The article fits well into contemporary research on
natural language engineering often considering corpus-training as a kind of
solution for all problems ignoring the scarcity of resources on the other hand
and the much higher requirement of unannotated resources for morphologically
complex languages on the other.

In \textbf{related works}, the concept of lesser resourced languages in
computational linguistics has come into focus in recent years. Particularly I
followed Anssi Yli-Jyrä's recent work on African
languages~\citep{yli2005toward} and all the work related to the previous years
of the Speech and Language Technology for Minority Languages workshop, where I
presented this paper.  The concept of using Wikipedia as a basis for a
statistical language model was likewise new common theme in the LREC
conferences.

The most central \textbf{results} of this work show that the Wikipedias for
small language groups will give some gains in spelling correction, when
properly coupled with a rule-based language model. This is despite the fact
that Wikipedias of lesser resourced language are comparably tiny and not very
high quality, especially when related to the quality of correct spelling.  I
have hypothesised that the underlying reason for this is that in the events
where simple spell\-/checking like edit distance or confusion sets produce
ambiguous results, in the majority of the cases it is beneficial to simply vote
for the most common word-form from these results.

As an additional result in this article I show how to use the language model to
automatically generate the baseline error models. This finding is expanded in
Section~\ref{chap:efficiency}.

In general the concept of using freely available and open sourced---and
crowd-sourced---data should be a common focus in the research of computational
linguistics in lesser resourced languages. This is one of the main sources of
large amounts of free data that most communities are willing to put an effort
into, and it does not require advanced algorithms or an expert proofreader to
make the data usable for many of the applications of computational linguistics.

\section{Conclusions}

In this chapter, I have discussed about some different approaches to creating
statistical language models for finite\-/state spell\-/checking and. I have
shown in this chapter that it is possible to take arbitrary language models for
morphologically complex languages and train them with same approaches
regardless of the method and theory according to which original model was
built. I have extended the basic word-form unigram training with methods from
n-gram training to be able to cope with infinite compounding of some
morphologically complex languages and thus laid out a path forward for all
morphologically complex languages to be trained with limited corpora.


\chapter{Error Models}
\label{chap:error-models}

The error modeling for spelling correction is an important part of a
spell\-/checking system and in my a opinion deserves to be treated as a whole
separate subtopic of its own. The finite\-/state formulation of error models in
a finite\-/state spell\-/checking system in particular is a relatively novel
idea, with only some theoretical works
\citep{agata2002typographical,mohri2003edit} and single examples in
books~\citep{beesley2003finite} actually dealing with it.  The error modeling
however has been a central topic for software-based solutions to
spell\-/checking,
cf.~\citet{kukich1992spelling,mitton2009ordering,deorowicz2005correcting}, so I
believe that a proper finite\-/state solution is called for.

One of the typical arguments for not using finite\-/state error models is that
special software algorithms are faster and more efficient than building and
especially applying the finite\-/state models. In the articles where I build
the finite\-/state models for spell\-/checking I have run some evaluations on
the resulting system to prove that the finite\-/state system is at usable speed
and memory consumption levels. A thorough evaluation of this, however, is in
Chapter \ref{chap:efficiency}.

The theoretical and practical motivation for suggesting finite\-/state models
for error correction is obvious: the expressive power of weighted two-tape
automaton is an ideal format for specifying the combinations of the spelling
errors that can be made and corrected, and since weighted finite\-/state
automata are closed under finite\-/state algebra it is easy and theoretically
clean to build even complex statistical models consisting of different types of
errors by combining them from building blocks. The fact that traditional
software based spelling correctors use a number of different algorithms
starting from all the variants and modifications of the edit distance to
arbitrary context based string rewriting to phonemic folding schemes provides a
good motivation to have such arbitrarily combinable systems for creating these
error models outside the software.

The rest of this chapter is laid out as follows: In
Section~\ref{sec:error-application} I describe how the finite\-/state
application of error modeling is implemented in our spelling correction system,
and in Section~\ref{sec:edit-distance} I show how our variant of finite\-/state
edit distance style error models are constructed.  In
Section~\ref{sec:Hunspell-error} I go through the existing error models used in
the popular spelling software \texttt{Hunspell}, and show their finite\-/state
formulations. In Section~\ref{sec:phonemic} I show how to build and apply the
phonemic key type of error models in finite\-/state form.  In
Section~\ref{sec:context} I discuss the context-based language models in
finite\-/state spelling correction for morphologically complex languages with
limited resources that is, mostly the limitations and necessary modifications.
Finally in Section~\ref{sec:other-errors} I go through some other error
correcting models and their finite\-/state formulations.

\section{The Application of Finite-State Error Models}
\label{sec:error-application}

There are many approaches to spelling correction with finite\-/state automata.
It is possible to use various generic graph algorithms to work on fuzzy
traversal of the finite\-/state graph, like is done in~\citet{hulden2009fast}.
It is also possible to create a finite\-/state network that already includes
the potential error mapping, as is shown in~\citet{schulz2002fast}. The
approach that I use is to apply basic finite\-/state algebra with a two-tape
automata working as the error model. This is a slight variation of a model
originally presented by~\citet{mohri2003edit}. We apply the errors to
misspelled strings by two compositions, performed serially in a single
operation: $\mathcal{M}_{Sugg} = (\mathcal{M}_{input} \circ \mathcal{M}_{error}
\circ \mathcal{M}_{language})_1$.  This specific form of finite\-/state error
correction was introduced in~\citepalias{pirinen2010finitestate}, but the
underlying technology and optimisations were more widely documented in
\citet{linden2011hfst}.

The implication of using weighted finite\-/state algebra is two-fold. On the
other hand we have full expressive power of the weighted finite\-/state
systems.  On the other hand, the specialised algorithms are known to be faster,
e.g.  the finite\-/state traversal shown by~\citet{hulden2009fast} is both fast
and has the option to have regular languages as contexts, but no true weights.
So there is a trade-off to consider when choosing between these methods, but
considering the popularity of statistical approaches in the field of natural
language engineering the time trade-off may be palatable for most end-users
developing language models.

The implication of the fully weighted finite\-/state framework for
spell\-/checking is that we can e.g. apply statistics to all parts of the
process, i.e., the probability of input, the conditional probability of a
specific combination of errors and the probability of the resulting word-form
in context. 

\section{Edit Distance Measures}
\label{sec:edit-distance}

The baseline error model for spelling correction since its inception in 1960s
has been Damerau-Levenshtein edit
distance~\citep{damerau1964technique,levenshtein1966binary}. There has been
multiple formulations of the finite\-/state edit distance, but in this section
we describe the one that is used by our systems. This formulation of the edit
distance as a two-tape finite\-/state automaton was first presented
in~\citet{schulz2002fast}.

Finite-state edit distance is a rather simple concept; each of the error types
except swap is represented by a single transition of the form $\epsilon:x$ (for
insertion), $x:\epsilon$ (for deletion) or $x:y$ (for change). The swap
requires an additional state in the automaton as a memory for the symbols to be
swapped at the price of one edit that is a path $\pi_{x:y y:x}$. This
unweighted edit distance 1 error model is shown in Figure~\ref{fig:edit1-ab}.

\begin{figure}
    \includegraphics[width=\textwidth]{edit1-ab-unw}
    \caption{Unweighted edit distance 1 with $\Sigma = {a, b}$.
    \label{fig:edit1-ab}}
\end{figure}

There are numerous possibilities to formalise the edit-distance in weighted
finite\-/state automata. The simplest form is a cyclic automaton that is
capable of measuring arbitrary distances, using the weights as a distance
measure. This weighted automaton for measuring edit distances is shown in
Figure~\ref{fig:edit-weighted}. This form is created by drawing arcs of the
form $\epsilon:x::w$, $x:\epsilon::w$ and $x:y::w$, from the initial state to
itself, where $w$ is weight of a given operation. If the measure includes the
swap of adjacent characters, the extra states and paths of the form $\pi_{x:y
y:x::w}$ need to be defined. It is possible to use this automaton to help
automatic harvesting or training of error model. The practical weighted
finite\-/state error models used in real applications are of the same form as
the unweighted ones defined earlier, with limited maximum distance, as the
application of the infinite edit measure is expectedly slow.

\begin{figure}
    \includegraphics[width=\textwidth]{edit-weighted}
    \caption{Weighted automaton for measuring arbitrary edit distance with
        $\Sigma = {a, b}$ and homogeneous weights.
    \label{fig:edit-weighted}}
\end{figure}

\citet{mohri2003edit} describes mathematical backgrounds of implementing a
weighted edit distance measure for finite\-/state automata. It differs slightly
from the formulation we use in that it describes edit distance of two weighted
automata exactly, whereas our definition is suitable for measuring edit
distance of a string or path automaton and the target language model. The
practical differences between their formulations and automata I presented
earlier seem to be minimal, e.g. placing weights into the final state instead
of the arcs.

\section{Hunspell Error Correction}
\label{sec:Hunspell-error}

\textbf{Main Article}: \emph{Building and Using Existing Hunspell Dictionaries
and \TeX\ Hyphenators as Finite-State Automata} by myself and Krister Lindén.
In this article We have formulated the Hunspell's software-based algorithms for
error correction and their specific optimisation tricks in terms of
finite\-/state automata.

The main \textbf{motivation} for this research was to reimplement Hunspell's
spell\-/checking in a finite\-/state form. In terms of the  error modeling it
basically consists of a modifications of edit distance for speed gains. The
nature of these modification are based on some popular findings on the spelling
errors that are made. One modification is to take the keyboard layout into
account when considering the replacement type of errors. Another addition is to
sort and limit the alphabet used for other error types. Finally Hunspell allows
the writer of the spell\-/checking dictionary to define specific
string-to-string mutations for common errors.

The finite\-/state formulation of these errors is a combination of limitations
and modifications to the edit distance automaton, disjuncted with a simple
string-to-string mapping automaton. The key aspect of Hunspell's modified
edit distance formulations are optimising the speed of the error lookup using
configurable, language specific, limitations of the edit distance algorithm's
search space. The other correctional facilities are mainly meant to cover the
type of errors that cannot be reached with regular edit distance modifications.
The effects of these optimisations are further detailed in the 
Chapter~\ref{chap:efficiency}.

The settings that Hunspell gives to a user are the following: A \texttt{KEY}
setting to limit characters that can be \emph{replaced} in the edit distance
algorithm to optimise the replacements to adjacent keys on a typical native
language keyboard. A \texttt{TRY} setting to limit the possible characters
that are used in the insertion and deletion part of the edit distance
algorithm. A \texttt{REP} setting that can be used for arbitrary
string-to-string confusion sets, and a parallel setting called \texttt{MAP}
used for encoding differences rather than linguistic confusions.  Finally, for
the English language, there is a \texttt{PHONE} setting that implements some
variation of the double metaphone algorithm~\citep{philips2000double}.

\section{Phonemic Key Corrections}
\label{sec:phonemic}

The phonemic keys are commonly used for languages where orthography does not
have a very obvious mapping to pronunciation, such as English. In the phonemic
keying methods the strings of a language are intended to connect with phonemic
or phonemically motivated description. This technique was originally used for
sorting family names in archives~\citep{russell1918soundex}. These systems have
been successfully adapted to spell\-/checking and are in use for English for
example in aspell as the double metaphone algorithm~\citep{philips2000double}.
Since these algorithms are a mapping from strings to strings, it is trivially
modifiable into a finite\-/state spelling correction model. Considering a
finite\-/state automaton $\mathcal{M}_{phon}$ that maps all strings of a
language into a single string called a phonetic key, the spelling correction
can be performed with mapping $\mathcal{M}_{phon} \circ
\mathcal{M}_{phon}^{-1}$.  For example, a soundex automaton can map strings
like \misspelt{squer} and \emph{square} into string \texttt{S140}, and the
composition of S140 to inverse of the soundex mapping would produce, among
others, \emph{square}, \misspelt{squer}, \misspelt{sqr}, \misspelt{sqrrr} and
so forth. And this set can be applied to the language model collecting only the
good suggestions, such as \emph{square}.

\section{Context-Aware Spelling Correction}
\label{sec:context}

\textbf{Main article:} \emph{Improving finite\-/state spell-checker suggestions
with part of speech n-grams} by myself, Miikka Silfverberg, and Krister
Lindén. In this article I have made an attempt to cover spelling correction
of morphologically complexer languages with finite\-/state technology.

The \textbf{motivation} for this article was to show that a finite\-/state
version of spell\-/checking is also usable for a context-aware form, and
secondarily to explore the limitations and required changes for the
context-based language models that are needed for morphologically complexer
languages to get similar results as are attained with simple statistical
context models for morphologically poor languages.

The \textbf{related works} include the well-known results on English for the
same task implemented in software approaches.  In general, the baseline of the
context-based spelling correction was probably established
by~\citet{mays1991context} and has been revisited many times, e.g.
in~\citet{wilcox-ohearn2008realword}, and usually found to be beneficial for
the languages that have been studied. The examples I have found are all of
morphologically poor languages such as English or Spanish. In my research I
have tried to replicate these results for Finnish, but the results have not
been as optimistic as with English or Spanish for example.  The basic word-form
n-grams were not seen as an efficient method for Spanish in
e.g.~\citet{otero2007contextual}, instead the method was extended by studying
also the part-of-speech analyses to rank the suggestion lists for a
spell-checker. In my article~\citepalias{pirinen2012improving} we reimplemented
a finite\-/state system very similar to the one used for Spanish, originally
unaware of these parallel results.

\textbf{Results} show that the regular n-grams, which are used for English to
successfully improve the spelling correction, are not so effective for Finnish.
Furthermore the improvement that is gained for Spanish when applying the POS
n-grams in this task does not give equally promising results for Finnish, but
do improve the quality of suggestions, by around 2~\% units.

One of the negative results of the paper is that the time and memory
consumption of the POS-based or word-form-based spelling correction is not yet
justifiable for general consumer products as a good tradeoff. There have been
many recent articles exploring the optimisatory side of the context-based
spell\-/checking that should be seen as the future work for context-based
finite\-/state spelling correction. The optimisation of the n-gram models has
been brought up many times in non-finite\-/state solutions too,
e.g.~\citep{church2007compressing}. While mainly an engineering stunt, there is
a lot to be desired from this development before practical end-user
applications for contextual spell\-/checking can be considered.

\section{Other Finite-State Error Models}
\label{sec:other-errors}

In~\citet{deorowicz2005correcting} there is an extensive study on error
modeling for spelling correction. Some of the models have been
introduced earlier in this chapter.

One of the optimisation tricks used in some practical spell\-/checking systems,
but not in Hunspell, is to avoid modifying the first character of the word in
the error modeling step. The gained improvement in speed is notable. The
rationale for this is the assumption that it is rarer to make mistakes in the
first character of the word---or perhaps easier to notice and fix such
mistakes. There are some measurements on this phenomenon
in~\citet{bhagat2007spelling}, but the probabilities of having the first-letter
error in papers he refers to vary from 1~\% up to 15~\%, so it is still open to
debate. In measurements I made for morphologically complex languages
in~\citepalias{pirinen2012improving}, I also found a slight tendency towards
the effect that the errors are more likely in non-initial parts of the words,
accounting for word length effects.

The concept of creating an error model from a (possibly annotated) error corpus
is doable for finite\-/state spelling-correction as well. The modification of
the training logic as presented by \citep{church1991probability} into
finite\-/state form is mostly trivial. E.g. if we consider the building of a
finite\-/state language model from a corpus of word-forms, the corpus of errors
is same set of string pairs with weights of $w(x:y) := -\log\frac{f(x:y) +
\alpha}{\mathcal{D}_{a:b \in \Sigma^2} f(a:b) + \alpha}$, for error pairs $x:y$
in a set of errors $\mathcal{D}$.

\section{Conclusions}

In this chapter I have enumerated some of the most important finite-state
formulations of the common error models found in spell-checkers and shown that
they are implementable and combinable under basic finite-state algebra. I have
re-implemented full set of error models used by current de-facto standard
spelling corrector Hunspell in a finite state form demonstrating that it is
possible to replace software-based correction algorithms by a sound
finite-state error model.

\chapter{Efficiency of Finite-State Spell-Checking}
\label{chap:efficiency}

One of the motivations for finite\-/state technology is that the time
complexity of the application of finite\-/state automata is known to be linear
with regards to size of the input as long as finite\-/state automata can be
optimised by operations like determinisation. In practice, however, the speed
and size requirements of finite\-/state automata required for language models,
error models and their combinations are not easy to predict, and the worst case
scenarios of these combinatorics specifically are still theoretically
exponential, because the error models and language models are kept separated
and composed during run time due to space restrictions. This is especially
important for error modelling in spell\-/checking, since it easily goes towards
the worst case scenario, i.e. the theoretically ideal naive error model that
can rewrite any string into any string with some probabilities attached is not
easily computable.

Towards the latter parts of my research on finite\-/state spell\-/checking I
set out to write a few larger scale evaluation articles to prove the efficiency
and usability of finite\-/state spell\-/checking. For larger scale testing I
also sought to show how the linguistically common but technologically almost
undealt with concept of morphological complexity practically relates to the
finite\-/state language models.

The evaluation of the finite\-/state spelling correction needs to be contrasted
with the existing comparisons and evaluations of spell\-/checking systems.
Luckily, the spell\-/checking has been a topic popular enough to have a number
of recent evaluations available.  Also the very informal introduction to
spell\-/checking by~\citet{norvig2010howto} has been a valuable source of
practical frameworks for a test setup of the evaluation of the spelling
checkers as well as the ultimate baseline comparison for English language
spell\-/checking.

The evaluation of the finite\-/state spelling checkers in this chapter consists
of two different goals: when working with English, we merely aim to reproduce
the results that are published in the literature and are well known. When
working with morphologically complexer languages, we aim to show that the more
complex languages we select, the more work and optimisations are needed to
attain reasonable performance for baseline spell\-/checking and correction.
Throughout the chapter we have tried to follow the linguistically motivated
selection of languages, i.e. selecting at least one from the moderately rich
and one from the polyagglutinative languages; the suitable languages for these
purposes having open source, freely available implementations have been found
in University of Tromsø's server\footnote{\url{http://giellatekno.uit.no}}.
From there we have selected North Saami as a moderately rich and Greenlandic as
an example of a polyagglutinative language.

The efficiency evaluation is split into two sections, in
Section~\ref{sec:speed} I evaluate the speed efficiency and optimisation
schemes of finite\-/state spelling checkers together with Sam Hardwick who did
the most of the legwork on the finite\-/state spell\-/checking implementation
in e.g.  \citep{linden2011hfst}. Then in Section~\ref{sec:quality} I measure
the effects these optimisations have on the quality of the spell\-/checking.

\section{Speed of Finite-State Spell-Checking}
\label{sec:speed}

\textbf{Main article:} \emph{Effect of Language and Error Models on Efficiency
of Finite-State Spell-Checking and Correction} by myself and Sam Hardwick. In
this article we set out to prove that finite\-/state spell\-/checking is an
efficient solution and document how builders of the spellers can tune the
parameters of their models.

The \textbf{motivation} for the article was practical, in discussions with
people building alternative spelling checkers, as well as with alpha testers of
our more complex spell-checkers it was commonly noted that the speed might be
an issue for finite\-/state spell\-/checking systems to gain ground from the
traditional solutions. It is noteworthy, that the formulation of the system was
from the get-go built so that the developers of the language and error models
can freely make these optimisations as they need.

When discussing such practical measurements as the speed of spell-checker, care
must be taken in evaluation metrics. The most common use of a spell-checker is
still inter-active, where a misspelt word is underlined when user makes a
mistake, and the system is capable of suggesting corrections when a user
requests one. This sets the limits that system should be capable of checking
text as it is typed, and responding with suggestion as they are demanded. It is
well-known that finite\-/state language models being single tape automata,
language recognisers, can answer the question of whether the string is in given
language in linear time. In practical measurements this amounts to
spell\-/checking times up to 100,000~strings per second e.g.
in~\citet{silfverberg2009hfst}. The central evaluation and optimisation was
therefore performed on the error correction part which varies both with the
language model and the error model selected.

In the article we study different \textbf{related} optimisation schemes from
non-finite\-/state spell\-/checking and evaluate the how different error models and
parameters affect the speed of finite\-/state the spell\-/checking. We also ran the
tests on scale of morphologically different languages to see if the concept of
morphological complexity reflects on the speed of spell\-/checking.

The optimisation tricks of speed in spell\-/checking software has been a central
topic in the research since the beginning. There are numerous solutions that we
ported into finite\-/state form to test their efficiency, and few a methods
related to problems with the finite\-/state formulation of the error models (i.e.
the generated duplicate correction paths in the edit distance models larger
than 1 edits long, and limitation of result set size during traversal).

One interesting optimisation that we studied was the ignoring of the errors at
the first-position of the word. This is known from many practical systems, but
scarcely documented. Some of the research, such as~\citet{bhagat2007spelling},
give some statistics on plausibility of such method that it is more likely to
make a mistake in other parts of the words than the very first letter, but do
not offer a hypothesis as to why this happens.  Practically, we replicated
these results for our set of morphologically correct languages as well, giving
further evidence that it may be used as an optimisation scheme with a
reasonable quality trade-off.

It is likewise interesting that even though we tried to devise methods to
prevent the larger error models of doing unnecessary things like removing and
adding the precisely same characters in succession, this did not provide
significant addition in the search speed of the corrections.

The comparative evaluation of the different language models is relatively
rarely found in the literature. This could be due to the fact that scientific
writing in spell\-/checking is still more oriented towards algorithmics
research, where the language models of related systems do not have an influence
on the asymptotic speed of the process and are thus considered uninteresting.
Most of the studies are focused on measuring overall systems of single
languages, such as Arabic~\citep{attia2013improved}, Indian
languages~\citep{chaudhuri2002towards}, Hungarian~\citep{tron2005hunmorph} or
Spanish~\cite{otero2007contextual}, as well as all the research on
English~\citep{mitton1987spelling}.

\section{Precision of Finite-State Spell-Checking}
\label{sec:quality}

\textbf{Main article:} \emph{Speed and Quality Trade-Offs in Weighted
Finite-State Spell-Checking}. In this article I take a broad look on the
evaluation of all the language and error models and testing approaches we have
developed in the past years of research and compare them with the existing
results on real systems.

The \textbf{motivation} for this article was to perform a larger survey of all
the finite\-/state spelling methods that have been implemented in my thesis
project. In the paper I aim to show that the current state-of-the-art for
finite\-/state spell\-/checking is starting to be a viable option for many
common end-user applications. This article is a practical continuation of the
speed optimisation evaluation, where I studied the concept of speeding up the
finite\-/state spell\-/checking systems by manipulating the error and language
models. In order for these optimisations to be usable for end-users I needed to
show that the quality degradation caused by the optimisation schemes that
remove a part of the search space is not disproportionately big for the error
models that are effective enough to be used in the real world systems.

When measuring the quality of spelling error correction, the basic finding is
that the simple edit-distance measure at length one will usually cove ther
majority of errors in typical typing error situations. For English the original
statistics were at 95~\%~\citep{damerau1964technique}, with various newer
studies giving similar findings between 79~\% and 95~\% depending on the type
of corpora~\citep{kukich1992spelling}.  The numbers for morphologically complex
languages are not in the same range, rather the edit distance one gains around
50--70~\% coverage. One reason for this may be that the length of a word is a
factor in the typing error process; e.g. such that it is more likely to type
two or more typos and leave them uncorrected with a language where the average
word length is 16 compared with one where it is 6. Intuitively this would make
sense, and the figures obtained from my tests in the article provide some
evidence to that effect.

\section{Conclusions}

In this chapter I have studied the aspects of finite\-/state spell\-/checking
that make it a plausible end-user spell-checker in a practical application. I
have shown that the decrease in speed is negligible when moving from an
optimised specific algorithmic methods to correct strings or traverse
finite\-/state network to an actual finite\-/state algebra is negligible. The
speed of finite-state solutions for correcting morphologically complex
languages is faster than that of current software solutions such as Hunspell.

I have tested a set of morphologically different languages and large corpora to
show the usability of finite\-/state language and error-models for practical
spell\-/checking applications. I have also shown that it is possible to tune
the systems for interactive spell\-/checking with little effort using the
finite\-/state algebra.

The practical findings show the requirement of different speeds and quality
factors for different applications of the spelling checker. The research
presented here shows neatly, that by varying both the automata that make up the
language model and the error model of the spelling checker it is possible to
create suitable variants of the spelling checkers for various tasks by simple
fine-tuning of the parameters of the finite\-/state automaton.

\chapter{Conclusion}
\label{chap:conclusion}

In this thesis I have researched the plausibility of making finite\-/state
language and error models for real-world, finite\-/state spell\-/checking and
correction applications, and presented approaches to formulate traditional
spell\-/checking solutions for finite\-/state use. The contributions of this
work are split into two separate sections:
Section~\ref{sec:scholarly-contributions} summarises the scientific
contributions of the work that is in my thesis, and
Section~\ref{sec:practical-contributions} recounts the results of this thesis
in terms of real-world spelling checkers that can be used in end-user systems.
Finally, in \ref{sec:future-work} I go through the related research questions
and practical ideas that the results of this thesis lead into.

\section{Scholarly Contributions}
\label{sec:scholarly-contributions}

One of the main contributions, I believe, is the improvement and verification
of the statistical language modeling methods in context of weighted
finite\-/state automata. I believe that throughout the thesis, by continously
using the initial finding of the word-form-based compound word training
approach introduced in~\citepalias{pirinen2009weighted}, I have shown that the
morphologically complex languages can be statistically trained as weighted
finite\-/state automata and I maintain that this approach should be carried on
to the majority of future finite\-/state language models.

In the spelling correction mechanics, I have stuck to the two-tape
finite\-/state automaton as an ubiquitous error model for finite\-/state
spelling correction, and shown that it can be reasonably used in an interactive
spelling correction software in place of any traditional spelling correction
solution.


As a contribution that is an almost practical software engineering side of
language modeling, I have shown that a generalised finite\-/state formula based
on knowledge of morphology can be used to compile the most common existing
language models into automata.

Finally, to tie in the scientific evaluation of finite\-/state spell\-/checking
as a viable alternative, I have performed a large-scale testing on a wide range
of morphologically different languages, with different resources, and shown
that all are plausibly usable for typical spelling correction situations.


\section{Practical Contributions}
\label{sec:practical-contributions}

The spell\-/checking software that is the topic of this thesis, is a very
practical real world application that, despite this being an academic
dissertation, cannot be ignored, when talking about the contributions of this
thesis. One of the important contributions, I believe, is the alpha testing
version of the Greenlandic spelling correction for OpenOffice, providing a kind
of a proof of concept that it is implementable.

The real world tool chain, consisting of libvoikko, enchant, and the GNOME
desktop, as well as the OpenOffice.org / Libreoffice and Firefox browser, has
been a good testing ground for other morphologically complex languages,
including Finnish and North Saami, but also the slew of forthcoming Uralic
languages currently being implemented at the University of Helsinki, most of
which have not ever had a spell\-/checking system at all. 

The statistical and weighted methods used to improve the language models have
been directly ported into classical tool chains and work flows of the 
finite\-/state morphology, and are now supported in the HFST tools including
\texttt{hfst-lexc} for traditional finite\-/state morphologies.

\section{Future Work}
\label{sec:future-work}

In terms of error correcting, the systems are deeply rooted in the basic models
of typing errors, such as Levenshtein-Damerau edit distance. It would be
interesting to try to implement more accurate models of errors based on
findings of cognitive-psychological studies on the kinds of errors that are
actually made---this path of research I believe is only scratched on the
surface with the optimisations like \emph{avoid typing mistakes at the
beginning of the word}. Furthermore, it should be possible to make adaptive
error models using a simple feedback system for the finite\-/state
error-models.

During the time I spent writing this thesis, the world of spell\-/checking has
considerably changed with the mass market of a variety of input methods in the
new touch screen user interfaces. When I started the thesis work, by far the
most common methods to input text were regular 100+ key PC keyboards and key
pads on mobile phones with T9 input method\footnote{\url{http://www.t9.com}}.
This is no longer the case, as input methods like Swype, XT9, and so on, are
gaining in popularity. A revision of the methods introduced in this thesis is
required, especially so since the input methods of these systems are
\emph{more} heavily reliant on the high quality, automatic spelling correction
than before\footnote{A case in point, there are collections of humorous
automatic spelling corrections caused by these systems, floating around on the
internet, such as \url{http://www.damnyouautocorrect.com/}.}.  This point will
slightly affect the considerations of the practical evaluations of the speed of
spell\-/checking; the new spelling correctors coupled with predictive entry
methods need to be called after each input event instead of only when
correction is required!

On the engineering and software management side, one very visible, practical,
perhaps even foreseeable future work, there would not be anything stopping
these methods from getting into use in real world spell-checkers. The actual
implementation has been done with the help of spell\-/checking systems like
voikko, and enchant. The morphologically complex languages that are one selling
point of this thesis are lacking a basic language technology support partially
due to the dominance of too limited string-based forms of language modeling.

The concept of context-aware spell\-/checking with finite\-/state systems was
merely scratched on the surface by the article
\citepalias{pirinen2012improving}, the results showed some promise and some
problems, but with some work it may be possible to get as sizable improvements
for morphologically complex languages as for morphologically simple ones. One
possible solution to be drawn from the actual results of the other articles in
the thesis is to approach in using morph-based statistics for all the languages
to get the possibly more robust statistics and more similar results. In order
to get the speed and memory efficiency to a level that is usable in everyday
applications, I suspect there is room for much improvement by basic engineering
decisions and optimisations. This has also been a topic in much of recent
spell\-/checking research for non-finite\-/state systems, e.g.
in~\citep{carlson2001scaling}.

The concept of context-based error-detection, the real-word error detection,
and other forms of grammar correction has not been dealt of a very large-scale
in this thesis. That is an exclusion I have done partly on purpose, but in fact
the basic context-aware real-word error detection task does not really differ
in practice from non-finite\-/state versions of it, and the conversion is even
more trivial than in the case of the error correction part shown in the article
\citepalias{pirinen2012improving}. The main limiting factor for rapidly testing
such a system is the lack of error corpora with real-word spelling errors.

As an theoretically interesting development, it would be nice to have a formal
proof that the finite\-/state spell\-/checking and correction models are a proper
superset of the Hunspell spell\-/checking methods, as my quantitative
evaluation of the current approach shows that this is plausible.

\bibliographystyle{apalike}
\bibliography{diss,arts}

\end{document}
% vim: set spell:
