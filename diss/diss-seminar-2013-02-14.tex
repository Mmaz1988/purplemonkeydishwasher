\documentclass[t,12pt]{beamer}
\usepackage{helvet}
\usepackage{times}
\usepackage{courier}

\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{textpos}
\usepackage{xspace}
\usepackage{array}

\graphicspath{{./fig/}}


% theme options: hy/ml/hum, rovio/sinetti, hiit
% default: hy,rovio

%\usetheme[hy]{HY}
%\usetheme[hy,sinetti]{HY}
\usetheme[hum,rovio]{HY}
%\usetheme[ml,rovio]{HY}
%\usetheme[ml,rovio,hiit]{HY}


\title{Weighted Finite-State Methods in Spell-Checking\\
\scriptsize{thesis status report in
research seminar}}
\author{Tommi A Pirinen \scriptsize \guilsinglleft{}tommi.pirinen@helsinki.fi\guilsinglright{}}
\institute{University of Helsinki\\Department of Modern Languages}
\date{\today}

\begin{document}

\selectlanguage{english}

\HyTitle
%\maketitle

\begin{frame}
    \frametitle{Outline (of Thesis)}
    \tableofcontents
\end{frame}

\section{Introduction, backgrounds, motivations, history, articles}

\begin{frame}
    \frametitle{Finite-State + Spell-Checking}
    \begin{itemize}
        \item A simple task: go through all words in text to see if they
            belong to language LL, if not, modify them with relation
            EE to fit into language LL
        \item In Finite-State world language model LL is any (weighted) single
            tape finite-state automaton recognising the words of the
            language
        \item The error model EE is any (weighted) two-tape automaton, that
            describes spelling errors, i.e. mapping from misspelt word into
            the correct one
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical, Real-World Motivation(s)}
    \begin{itemize}
        \item Fastest and most efficient way to deal with English is always
            finite data-structure of all word-forms (harvested from corpora)
            $\rightarrow$ ``this problem is solved / trivial''$\dagger$
        \item maybe finite-state approach, \emph{infinite} lexicons,
            and such may be necessary for morphologically complexer languages?
            \begin{itemize}
            \item e.g. cumulative amount of unique word-forms in texts of
            morphologically complex languages, the graphs for Finnish and
            English from Wiki are quite different
            \end{itemize}
        \item esp. lesser resourced languages won't get good spell-checker from
            corpora only
    \end{itemize}
    \begin{tiny}$\dagger$latest measure from my FSA English speller is only
    beaten by aspell but not hunspell.\end{tiny}
\end{frame}

\begin{frame}
    \frametitle{Theoretical Motivations?}
    \begin{itemize}
        \item Regular grammars or FSAs are the weakest to describe ~fully
            morph. complex natural langs? (Not provable)
        \item Formal langs or methods for subset of regular languages do not
            generally improve efficiency? In terms of computational complexity;
            experimentally..?
        \item WFSAs provide a neat framework for bit of probabilistic and ruled
            combinatorics of preferences in spell-checking task
    \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
  \frametitle{Thesis Articles}
  \begin{small}
      \phantomsection
      \bibliographystyle{apalike}
      \bibliography{diss}
    \end{small}
\end{frame}

\section{Language Models}

\begin{frame}
    \frametitle{Short history of Speller LMs:}
    \begin{enumerate}
        \item letter n-grams
        \item word-form list $\rightarrow$ \cite{pirinen2012effects}
        \item ispell, aspell, hunspell (stems, affix stripping)
            $\rightarrow$ \cite{pirinen2010building,pirinen2010creating}

        \item contexts (word-form trigrams, POSes) $\rightarrow$
            \cite{pirinen2012improving}
        \item finite-state automata $\rightarrow$ all cited
        \item statistical LMs (Bayspell, winnow spell, etc.)
            $\rightarrow$ \cite{pirinen2010finitestate}
    \end{enumerate}
\end{frame}

\section{Statistical Language Models}
\begin{frame}
    \frametitle{Short list of Weighted LMs:}
    \begin{itemize}
        \item surface word-form unigram probabilities
        \item rules based on lemmas and tags
        \item surface word-form n-grams
        \item analysis probabilities almost require disambiguated gold
            corpora
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{WF Probabilities(?) of morphologically complex Langs}
    \begin{itemize}
        \item long compounds, derivation chains etc. that exist in these
            langs get rarer in corpora
        \item Using word-forms to train compounds in Finnish: 
            weight of new compound \emph{foo}+\emph{bar} = weights of components
            foo, bar combined~\cite{pirinen2009weighted,pirinen2009weighting}
        \item Should be generalised: weights are counted per morph for for
            all languages $\rightarrow$ all languages become equally simple?
    \end{itemize}
\end{frame}

\section{Error Models}

\begin{frame}
    \frametitle{Short list of WFSA Error Models:}
    \begin{itemize}
        \item Levenshtein-Damerau Edit distance (keyboard typing mistakes)
            \cite{pirinen2010finitestate}
        \item its optimisations: no edit at first position, limiting distance,
            cutting parts of alphabet for mistakes
            \cite{pirinen2012effects}
        \item Confusion sets (competency errors); arbitrary string-to-string
            mappings \cite{pirinen2010building}
        \item Mistakes learnt from error corpora, may require manually verified
            good data
        \item Typical FSA ErrM would be a combination of all, done by simple
            disjuncting union join etc.
    \end{itemize}
\end{frame}


\section{Efficiency Evaluations}

\begin{frame}
    \frametitle{Speed measurements}
    \begin{table}
        \centering
        \begin{tabular}{|l|r|}
            \hline
            \bf System & WPS \\
            \hline
            \bf English Hunspell & 174 \\
            \bf English aspell & 20,000 \\
            \bf English WFSA & 999 \\
            \hline
            \bf North Saami Hunspell & 3 \\
                \bf North Saami WFSA & 22 \\
            \hline
            \bf Finnish aspell & 781 \\
              \bf Finnish WFSA & 1/3 \\
            \hline
            \bf Greenlandic WFSA & 1/3 \\
        \end{tabular}
    \caption{The speed of finite-state spell-checking~\cite{pirinen2012effects}}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{Quality measurements}
    \begin{table}
        \centering
        \begin{tabular}{|l|r|}
            \hline
            \bf System & Correct sug. 1st \\
            \hline
            \bf English Hunspell & 59.3~\%  \\
            \bf English aspell & 55.7~\% \\
            \bf English WFSA & 73.7~\%  \\
            \hline
            \bf Finnish aspell & 21.1~\% \\
            \bf Finnish WFSA & 54.8~\% \\
            \hline
            \bf North Saami Hunspell & 9.4~\%  \\
            \bf North Saami WFSA & 3.5~\% \\
            \hline
            \bf Greenlandic WFSA & 13.3~\% \\
        \end{tabular}
        \caption{The quality of spell-checkers in first suggestion correct
        statistics~\cite{pirinen2013quality}}
    \end{table}
\end{frame}



\end{document}
% vim: set spell:
